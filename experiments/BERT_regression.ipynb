{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 config는 추후 json 파일로 저장해놓기\n",
    "CONFIG = dict(\n",
    "    seed = 12345,\n",
    "    pretrained_model = 'bert-base-uncased',\n",
    "    output_dir = '../models/bert_regression_original',\n",
    "    train_file = '../data/4th/v0/train.csv',\n",
    "    dev_file = '../data/4th/v0/dev.csv',\n",
    "    train_batch_size = 64,\n",
    "    dev_batch_size = 64,\n",
    "    lr = 5e-5,\n",
    "    num_classes = 1,\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    '''toxic dataset for BERT regression\n",
    "    '''\n",
    "    def __init__(self, tokenizer:AutoTokenizer, file_path, dir_path, mode, force=False) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.dir_path = dir_path # output dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.force = force \n",
    "        self.inputs, self.labels = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "\n",
    "        if not os.path.isdir(self.dir_path):\n",
    "            os.mkdir(self.dir_path)\n",
    "\n",
    "        if not self.force and os.path.isfile(os.path.join(self.dir_path, f\"{self.mode}_X.pt\")):\n",
    "            # torch tensor를 불러오기\n",
    "            encodings = torch.load(os.path.join(self.dir_path, f\"{self.mode}_X.pt\"))\n",
    "            labels = torch.load(os.path.join(self.dir_path, f\"{self.mode}_Y.pt\"))\n",
    "        else:\n",
    "            # 새로 파일 만들고 싶을 때 기존의 파일 지움\n",
    "            if self.force and os.path.isfile(os.path.join(self.dir_path, f\"{self.mode}_X.pt\")):\n",
    "                os.remove(os.path.join(self.dir_path, f\"{self.mode}_X.pt\"))\n",
    "                os.remove(os.path.join(self.dir_path, f\"{self.mode}_Y.pt\"))\n",
    "\n",
    "            # read csv file\n",
    "            data = pd.read_csv(self.file_path)\n",
    "\n",
    "            encodings = self.tokenizer(text=data.comment.tolist(),\n",
    "                                       add_special_tokens=True,\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       return_attention_mask=True)\n",
    "\n",
    "            labels = data.score.to_numpy()\n",
    "\n",
    "            # save the tensor\n",
    "            torch.save(encodings, os.path.join(self.dir_path, f\"{self.mode}_X.pt\"))\n",
    "            torch.save(labels, os.path.join(self.dir_path, f\"{self.mode}_Y.pt\"))\n",
    "        \n",
    "        return encodings, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.inputs[idx, :, :], self.labels[idx]\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.size\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 12345):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataloader\n",
    "from torch import nn\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['pretrained_model'])\n",
    "train_dataset = RegressionDataset(tokenizer=tokenizer, file_path=CONFIG['train_file'], dir_path=CONFIG['output_dir'], mode='train')\n",
    "dev_dataset = RegressionDataset(tokenizer=tokenizer, file_path=CONFIG['dev_file'], dir_path=CONFIG['output_dir'], mode='dev')\n",
    "train_dataloader = Dataloader(train_dataset, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "dev_dataloader = Dataloader(dev_dataset, batch_size=CONFIG['dev_batch_size'], shuffle=False)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CONFIG['pretrained_model'], num_labels=CONFIG['num_classes'])\n",
    "\n",
    "model.to(CONFIG['device'])\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=CONFIG['lr'],\n",
    "                  eps=1e-8)\n",
    "\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, col):\n",
    "    '''\n",
    "    clean text\n",
    "    '''\n",
    "    # Clean some punctutations\n",
    "    # data[col] = data[col].str.replace('\\n', ' \\n ')\n",
    "    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n",
    "    # Replace repeating characters more than 3 times to length of 3\n",
    "    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n",
    "    # Add space around repeating characters\n",
    "    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n",
    "    # patterns with repeating characters \n",
    "    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n",
    "    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n",
    "    data[col] = data[col].str.replace(r'[ ]{2,}|\\n',' ')\n",
    "    # filter ibans(국제계좌형식)\n",
    "    # filter email\n",
    "    # filter websites\n",
    "    # filter phone number\n",
    "    # quotation marks\n",
    "    pattern = r'(fr\\d{2}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{2}|fr\\d{20}|fr[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}[ ]\\d{3}[ ]\\d{5})|' \\\n",
    "               '((?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\\\"[a-zA-Z0-9.+!% -]{1,64}\\\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,}))|' \\\n",
    "               '((https?:\\/\\/)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*))|' \\\n",
    "               '([0-9]+.[0-9]+.[0-9]+.[0-9]+)|' \\\n",
    "               '((?:(?:\\+|00)33[\\s.-]{0,3}(?:\\(0\\)[\\s.-]{0,3})?|0)[1-9](?:(?:[\\s.-]?\\d{2}){4}|\\d{2}(?:[\\s.-]?\\d{3}){2})|(\\d{2}[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}))|' \\\n",
    "               '\\\"'\n",
    "    data[col] = data[col].str.replace(pattern, '')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9da130fd146a48ad66e5fea3a395b562b6033577f697a813e60dca094d36e5b3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('jigsaw': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
