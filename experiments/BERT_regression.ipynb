{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 config는 추후 json 파일로 저장해놓기\n",
    "CONFIG = dict(\n",
    "    seed = 12345,\n",
    "    pretrained_model = 'bert-base-uncased',\n",
    "    output_dir = '../models/bert_regression_original',\n",
    "    train_file = '../data/4th/v0/train.csv',\n",
    "    dev_file = '../data/4th/v0/dev.csv',\n",
    "    train_batch_size = 128,\n",
    "    dev_batch_size = 64,\n",
    "    lr = 5e-5,\n",
    "    epochs = 5,\n",
    "    num_classes = 1,\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 12345):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    '''toxic dataset for BERT regression\n",
    "    '''\n",
    "    def __init__(self, tokenizer:AutoTokenizer, file_path, dir_path, mode, force=False) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.dir_path = dir_path # output dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.force = force \n",
    "        self.inputs, self.labels = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "\n",
    "        if not os.path.isdir(self.dir_path):\n",
    "            os.mkdir(self.dir_path)\n",
    "\n",
    "        if not self.force and os.path.isfile(os.path.join(self.dir_path, f\"{self.mode}_X.pt\")):\n",
    "            # torch tensor를 불러오기\n",
    "            encodings = torch.load(os.path.join(self.dir_path, f\"{self.mode}_X.pt\"))\n",
    "            labels = torch.load(os.path.join(self.dir_path, f\"{self.mode}_Y.pt\"))\n",
    "        else:\n",
    "            # 새로 파일 만들고 싶을 때 기존의 파일 지움\n",
    "            if self.force and os.path.isfile(os.path.join(self.dir_path, f\"{self.mode}_X.pt\")):\n",
    "                os.remove(os.path.join(self.dir_path, f\"{self.mode}_X.pt\"))\n",
    "                os.remove(os.path.join(self.dir_path, f\"{self.mode}_Y.pt\"))\n",
    "\n",
    "            # read csv file\n",
    "            data = pd.read_csv(self.file_path)\n",
    "            encodings = self.tokenizer(text=data.comment.tolist(),\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True)\n",
    "\n",
    "            labels = data.score.to_numpy()\n",
    "\n",
    "            # save the tensor\n",
    "            torch.save(encodings, os.path.join(self.dir_path, f\"{self.mode}_X.pt\"))\n",
    "            torch.save(labels, os.path.join(self.dir_path, f\"{self.mode}_Y.pt\"))\n",
    "        \n",
    "        return encodings, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.inputs[idx, :, :], self.labels[idx]\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.size\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    '''toxic dataset for BERT regression\n",
    "    '''\n",
    "    def __init__(self, tokenizer:AutoTokenizer, file_path, dir_path, mode, force=False) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.dir_path = dir_path # output dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.force = force \n",
    "        \n",
    "        # read csv file\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        self.labels = self.data.score.to_numpy()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(text=self.data.comment[idx],\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True)\n",
    "\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.size\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['pretrained_model'])\n",
    "train_dataset = RegressionDataset(tokenizer=tokenizer, file_path=CONFIG['train_file'], dir_path=CONFIG['output_dir'], mode='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "dev_dataset = RegressionDataset(tokenizer=tokenizer, file_path=CONFIG['dev_file'], dir_path=CONFIG['output_dir'], mode='dev')\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=CONFIG['dev_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CONFIG['pretrained_model'], num_labels=CONFIG['num_classes'])\n",
    "\n",
    "model.to(CONFIG['device'])\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=CONFIG['lr'],\n",
    "                  eps=1e-8)\n",
    "\n",
    "epochs = CONFIG['epochs']\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, col):\n",
    "    '''\n",
    "    clean text\n",
    "    '''\n",
    "    # Clean some punctutations\n",
    "    # data[col] = data[col].str.replace('\\n', ' \\n ')\n",
    "    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n",
    "    # Replace repeating characters more than 3 times to length of 3\n",
    "    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n",
    "    # Add space around repeating characters\n",
    "    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n",
    "    # patterns with repeating characters \n",
    "    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n",
    "    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n",
    "    data[col] = data[col].str.replace(r'[ ]{2,}|\\n',' ')\n",
    "    # filter ibans(국제계좌형식)\n",
    "    # filter email\n",
    "    # filter websites\n",
    "    # filter phone number\n",
    "    # quotation marks\n",
    "    pattern = r'(fr\\d{2}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{4}[ ]\\d{2}|fr\\d{20}|fr[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}[ ]\\d{3}[ ]\\d{5})|' \\\n",
    "               '((?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\\\"[a-zA-Z0-9.+!% -]{1,64}\\\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,}))|' \\\n",
    "               '((https?:\\/\\/)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*))|' \\\n",
    "               '([0-9]+.[0-9]+.[0-9]+.[0-9]+)|' \\\n",
    "               '((?:(?:\\+|00)33[\\s.-]{0,3}(?:\\(0\\)[\\s.-]{0,3})?|0)[1-9](?:(?:[\\s.-]?\\d{2}){4}|\\d{2}(?:[\\s.-]?\\d{3}){2})|(\\d{2}[ ]\\d{2}[ ]\\d{3}[ ]\\d{3}))|' \\\n",
    "               '\\\"'\n",
    "    data[col] = data[col].str.replace(pattern, '')\n",
    "    data = data.dropna(axis=0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/.pyenv/versions/3.7.6/envs/jigsaw/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Above user has no en talk page, but does have...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelson   Thanks for the tip ! Over at WP:RIGH...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOU FAT SLIMY PIECE OF SHIT   I HOPE YOU FALL ...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lots More Abstracts Smithsonian / NASA Astroph...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 May 2006  Please do not add nonsense to Wiki...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  score\n",
       "0   Above user has no en talk page, but does have...    0.0\n",
       "1  Adelson   Thanks for the tip ! Over at WP:RIGH...    0.0\n",
       "2  YOU FAT SLIMY PIECE OF SHIT   I HOPE YOU FALL ...    0.8\n",
       "3  Lots More Abstracts Smithsonian / NASA Astroph...    0.0\n",
       "4  1 May 2006  Please do not add nonsense to Wiki...    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('../data/4th/v0/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1778810"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "t = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding, list)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t(train_data['comment'][0], padding='max_length', truncation=True)\n",
    "type(a), type(a['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870400000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1700000 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2682, 5310, 2038, 2053, 4372, 2831, 3931, 1010, 2021, 2515, 2031, 2028, 2012, 2139, 1024, 3841, 20267, 2121, 9785, 17854, 3258, 1024, 21541, 10484, 2099, 1045, 2123, 1005, 1056, 2156, 1037, 3291, 2007, 2240, 1011, 7807, 1999, 1996, 3793, 1025, 2738, 1010, 2009, 1005, 1055, 2070, 5098, 10466, 2029, 2024, 4394, 1012, 1996, 18407, 2024, 3492, 2172, 2035, 2006, 7674, 1010, 2061, 2151, 4394, 2013, 4372, 2024, 3497, 4394, 2013, 2139, 2036, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = t.encode_plus(train_data['comment'][0], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Above user has no en talk page, but does have one at de:Benutzer Diskussion:Istiller I don ' t see a problem with line-breaks in the text; rather, it ' s some junction shapes which are missing. The icons are pretty much all on commons, so any missing from en are likely missing from de also. \",\n",
       " \"Adelson   Thanks for the tip ! Over at WP:RIGHT we could really use your help. Please consider becoming a part of the fastest growing most influential ensemblages of editors in the entire wiki: WP:WikiProject Conservatism / About us. – Lionel (talk)    Republican Party presidential candidates, 2012   I for some reason still have this page on my watchilist, and see you ' ve been quite busy here lately. I subsequently have had the page protected for a while. Hopefully that helps. 1992  Heh, thanks... I ' ve been resisting the impulse to hit RFPP over the single intermittent but persistently disruptive IP I ' ve been (along with others) reverting, but get the feeling from other edits over constant numbers-rigging that it can probably use the protection for several reasons I just threw up my hands at.\",\n",
       " 'YOU FAT SLIMY PIECE OF SHIT   I HOPE YOU FALL THROUGH THE TOILET YA JOBIE. YOU ARE ONE OF THE MPST UNEDUCATED DICKS I HAVE EVER HEARD OF, BURN IN HELL YOU BUGGER, BURN IN HELL ! PS. YOU ARE A SHIT.',\n",
       " 'Lots More Abstracts Smithsonian / NASA Astrophysics Data System (ADS) Query Results from the Instrumentation Database Retrieved 100 abstracts, starting with number 1. Total number selected: 637550. -  Bibcode   Score Date List of Links   Authors   Title   Access Control Help     1 1977OSAJ...67..399G    A C U   Grosso, R. P.; Yellin, M.   The membrane mirror as an adaptive optical element     2 1976SPIE...75...97Y    A T M   Yellin, M.   Using membrane mirrors in adaptive optics     3 1993SPIE.1945..421M    A T M U   Miller, Linda M.; Agronin, Michael L.; Bartman, Randall K.; Kaiser, William J.; Kenny, Thomas W.; Norton, Robert L.; Vote, Erika C.   Fabrication and characterization of a micromachined deformable mirror for adaptive optics applications     4 1991SPIE.1542..165C    A T M C   Clampin, M.; Durrance, S. T.; Golimowski, D. A.; Barkhouser, R. H.   The Johns Hopkins Adaptive Optics Coronagraph     5 2000SPIE.4091...83Y    A T M U   Yang, Eui-Hyeok; Wiberg, Dean V.; Dekany, Richard G.   Design and fabrication of electrostatic actuators with corrugated membranes for MEMS deformable mirror in space     6 2000SPIE.4075...41R    A T M U   Ross, Alan W.; Graham, Stephen C.; Gundlach, Alan M.; Stevenson, J. Tom M.; Hossack, William J.; Vass, David G.; Bodammer, Georg; Smith, Euan; Ward, Kevin   Microfabrication and packaging of deformable mirror devices     7 1994SPIE.2201..762T    A T M C U   Takami, Hideki; Iye, Masanori   Membrane deformable mirror for SUBARU adaptive optics     8 1999SPIE.3785..160W    A T M   Winsor, Robert S.; Sivaramakrishnan, Anand; Makidon, Russell B.   Finite element analysis of low-cost membrane deformable mirrors for high-order adaptive optics     9 2000SPIE.4348..348B    A T M   Borovkov, Alexei I.; Pyatishev, Evgenij N.; Lurie, Mihail S.; Korshunov, Andrey V.; Akulshin, Y. D.; Dolganov, A. G.; Sabadash, V. O.   Micronozzles: 3D numerical structural and gas dynamics modeling, fabrication, and preliminary experimental results     10 1999SPIE.3591..137Z    A T M   Zhu, Lijun; Sun, Pang Chen; Bartsch, Dirk-Uwe G.; Freeman, William R.; Fainman, Yeshaiahu   Adaptive fundus imaging using a micromachined membrane deformable mirror     11 2001SPIE.4327...13W    A T M C   Wagner, John W.; Agnes, Gregory S.   Optical metrology of adaptive membrane mirrors     12 2002OptEn..41..561P    A E F R U   Perreault, Julie A.; Bifano, Thomas G.; Levine, Bruce M.; Horenstein, Mark N.   Adaptive optic',\n",
       " '1 May 2006  Please do not add nonsense to Wikipedia. It is considered vandalism. If you would like to experiment, use the sandbox. Thank you.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['comment'][0:5].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9da130fd146a48ad66e5fea3a395b562b6033577f697a813e60dca094d36e5b3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
