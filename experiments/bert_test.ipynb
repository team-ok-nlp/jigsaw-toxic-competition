{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import clean, getData\n",
    "from data import DataProcessor\n",
    "from model import BertRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cofig\n",
    "CONFIG = dict(\n",
    "    seed = 12345,\n",
    "    pretrained_model = 'bert-base-uncased',\n",
    "    output_dir = '../models/bert_regression_mini',\n",
    "    train_file = '4th/v0/train.csv',\n",
    "    dev_file = '4th/v0/dev.csv',\n",
    "    train_batch_size = 32,\n",
    "    dev_batch_size = 32,\n",
    "    lr = 5e-5,\n",
    "    epochs = 5,\n",
    "    num_class = 1,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    device_ids = [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# init bert pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['pretrained_model'])\n",
    "model = BertRegressor(CONFIG['pretrained_model'], CONFIG['num_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4th/v0/train.csv ...\n",
      "Read 4th/v0/dev.csv ...\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# train dataset\n",
    "train_df = getData(data_path=CONFIG['train_file'])\n",
    "# data processing with tokenizing\n",
    "train_data = DataProcessor(train_df, tokenizer, is_eval=False)\n",
    "train_dataloader = DataLoader(train_data, batch_size=CONFIG['train_batch_size'], shuffle=True, num_workers=4)\n",
    "\n",
    "# dev dataset\n",
    "dev_df = getData(data_path=CONFIG['dev_file'])\n",
    "# data processing with tokenizing\n",
    "dev_data = DataProcessor(dev_df, tokenizer, is_eval=False)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=CONFIG['dev_batch_size'], shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train or Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataloader, dev_dataloader, criterion, optimizer, scheduler, device, output_dir):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_loss_train = 0.0\n",
    "\n",
    "            print(f\"[Epochs : {epoch_num+1}/{epochs}]\")\n",
    "            for i, (train_input, train_label) in enumerate(tqdm(train_dataloader)):\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                output = torch.squeeze(output, 1)\n",
    "                del input_id\n",
    "                del mask\n",
    "                \n",
    "                train_label = train_label.to(device)\n",
    "                batch_loss = criterion(output.float(), train_label.float())\n",
    "                del train_label\n",
    "\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                if i%10000 == 0:  \n",
    "                    print(f'Epochs: {epoch_num + 1} | Train Loss: {batch_loss: .3f}')\n",
    "                    torch.save(model.state_dict(),\\\n",
    "                            os.path.join(output_dir, f'bert_regression-{epoch_num+1}-{i}.pt'))\n",
    "\n",
    "            # validate using our dev set \n",
    "            model.eval()\n",
    "            total_loss_dev = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for dev_input, dev_label in dev_dataloader:\n",
    "                    dev_label = dev_label.to(device)\n",
    "                    input_id = dev_input['input_ids'].squeeze(1).to(device)\n",
    "                    mask = dev_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "                    output = torch.squeeze(output, 1)\n",
    "\n",
    "                    batch_loss = criterion(output.float(), dev_label.float())\n",
    "                    total_loss_dev += batch_loss.item()\n",
    "\n",
    "                    del dev_label\n",
    "                    del input_id\n",
    "                    del mask\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f} \\\n",
    "                | Val Loss: {total_loss_dev / len(dev_dataloader): .3f}')\n",
    "\n",
    "            torch.save(model.state_dict(),\\\n",
    "                    os.path.join(output_dir, f'bert_regression-{epoch_num+1}-{len(train_dataloader)}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertRegressor(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (regressor): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(CONFIG['output_dir'], 'model_ckpt.pt'))\n",
    "# checkpoint = torch.load(os.path.join(CONFIG['output_dir'], 'model_ckpt-55587.pt'))\n",
    "model.load_state_dict(checkpoint)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model, device_ids=CONFIG['device_ids'])\n",
    "model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4th/validation_cleaned.csv ...\n",
      "Read 4th/comments_to_score.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Validation data \n",
    "df_val = getData(data_path=\"4th/validation_cleaned.csv\")\n",
    "# Test data\n",
    "df_sub = getData(data_path=\"4th/comments_to_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1_data = DataProcessor(df_val['less_toxic'], tokenizer, is_eval=True)\n",
    "val1_dataloader = DataLoader(val1_data, batch_size=CONFIG['dev_batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "val2_data = DataProcessor(df_val['more_toxic'], tokenizer, is_eval=True)\n",
    "val2_dataloader = DataLoader(val2_data, batch_size=CONFIG['dev_batch_size'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_dataloader, device):\n",
    "\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tmodel.eval()\n",
    "\n",
    "\toutputs = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, val_input in enumerate(tqdm(val_dataloader)):\n",
    "\t\t\tinput_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\t\t\tmask = val_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "\t\t\toutput = model(input_id, mask)\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tprint(output.shape)\n",
    "\t\t\t#output = torch.squeeze(output, 1)\n",
    "\t\t\toutputs.extend(output.detach().cpu().numpy())\n",
    "\n",
    "\t\t\tdel input_id\n",
    "\t\t\tdel mask\n",
    "\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation\n",
    "- final validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict less toxic\n",
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 941/941 [02:47<00:00,  5.63it/s]\n",
      "  0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict more toxic\n",
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 941/941 [02:48<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "device = CONFIG['device']\n",
    "print('predict less toxic')\n",
    "p1 = predict(model, val1_dataloader, device)\t\n",
    "print('predict more toxic')\n",
    "p2 = predict(model, val2_dataloader, device)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.asarray(p1)\n",
    "p2 = np.asarray(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 66.23\n"
     ]
    }
   ],
   "source": [
    "print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5338fa82459e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "df_val['p1'] = p1\n",
    "df_val['p2'] = p2\n",
    "df_val['diff'] = np.abs(p2 - p1)\n",
    "df_val['correct'] = (p1 < p2).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>correct</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24702</th>\n",
       "      <td>204</td>\n",
       "      <td>This page is so conventional. It's sources are...</td>\n",
       "      <td>\"\\n\\nA more important point about Vuia is that...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.126276</td>\n",
       "      <td>0.126274</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>605</td>\n",
       "      <td>Well... D I cant say I will join in on the I w...</td>\n",
       "      <td>Sexism \\n\\nI've tried my hardest to explain t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040171</td>\n",
       "      <td>0.040115</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9703</th>\n",
       "      <td>515</td>\n",
       "      <td>Well... D I cant say I will join in on the I w...</td>\n",
       "      <td>Sexism \\n\\nI've tried my hardest to explain t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040171</td>\n",
       "      <td>0.040115</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>220</td>\n",
       "      <td>\"\\nI would like to agree with you, and maybe I...</td>\n",
       "      <td>THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150288</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>387</td>\n",
       "      <td>\"\\nI would like to agree with you, and maybe I...</td>\n",
       "      <td>THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150288</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4570</th>\n",
       "      <td>441</td>\n",
       "      <td>\"\\nI would like to agree with you, and maybe I...</td>\n",
       "      <td>THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150288</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26873</th>\n",
       "      <td>451</td>\n",
       "      <td>JLaTondre is a gay boy who likes to suck dick...</td>\n",
       "      <td>Thank You your the Wellcome\\nARABIC ASSHOLR GO...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363390</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>216</td>\n",
       "      <td>\"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...</td>\n",
       "      <td>Hey RGT: you're not just simply disagreeing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129691</td>\n",
       "      <td>0.129503</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>168</td>\n",
       "      <td>\"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...</td>\n",
       "      <td>Hey RGT: you're not just simply disagreeing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129691</td>\n",
       "      <td>0.129503</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>180</td>\n",
       "      <td>\"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...</td>\n",
       "      <td>Hey RGT: you're not just simply disagreeing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129691</td>\n",
       "      <td>0.129503</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>254</td>\n",
       "      <td>hiding behind these bitches wikipedians who ke...</td>\n",
       "      <td>You are an arrogant little so-and-so - aren't ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297834</td>\n",
       "      <td>0.297586</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>256</td>\n",
       "      <td>September 17th vandalized \\n\\nThis date which...</td>\n",
       "      <td>\"\\n\\nSorry, being a fan of wrestling I would a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>0.013654</td>\n",
       "      <td>0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20229</th>\n",
       "      <td>209</td>\n",
       "      <td>Absolutely hysterical.  Go get consensus on th...</td>\n",
       "      <td>Beckenham Library user blocked on public comp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071487</td>\n",
       "      <td>0.071217</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>302</td>\n",
       "      <td>\"\\n\\nRebuild of Evangelion revert warring\\n\\nT...</td>\n",
       "      <td>\"\\n\\n A barnstar for you! \\n\\n  The Original B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072787</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>300</td>\n",
       "      <td>\"\\n\\nRebuild of Evangelion revert warring\\n\\nT...</td>\n",
       "      <td>\"\\n\\n A barnstar for you! \\n\\n  The Original B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072787</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11725</th>\n",
       "      <td>572</td>\n",
       "      <td>User:Adam1213/warn&gt;\\n\\n Vandalism warnings</td>\n",
       "      <td>A Moldavian\\nI am a Moldavian, and I can expla...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>-0.009871</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11724</th>\n",
       "      <td>732</td>\n",
       "      <td>User:Adam1213/warn&gt;\\n\\n Vandalism warnings</td>\n",
       "      <td>A Moldavian\\nI am a Moldavian, and I can expla...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>-0.009871</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10382</th>\n",
       "      <td>489</td>\n",
       "      <td>This user has a history of sock puppetry and l...</td>\n",
       "      <td>\"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10381</th>\n",
       "      <td>171</td>\n",
       "      <td>This user has a history of sock puppetry and l...</td>\n",
       "      <td>\"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10380</th>\n",
       "      <td>22</td>\n",
       "      <td>This user has a history of sock puppetry and l...</td>\n",
       "      <td>\"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "24702     204  This page is so conventional. It's sources are...   \n",
       "9704      605  Well... D I cant say I will join in on the I w...   \n",
       "9703      515  Well... D I cant say I will join in on the I w...   \n",
       "4571      220  \"\\nI would like to agree with you, and maybe I...   \n",
       "4569      387  \"\\nI would like to agree with you, and maybe I...   \n",
       "4570      441  \"\\nI would like to agree with you, and maybe I...   \n",
       "26873     451   JLaTondre is a gay boy who likes to suck dick...   \n",
       "1934      216  \"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...   \n",
       "1935      168  \"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...   \n",
       "1936      180  \"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...   \n",
       "27670     254  hiding behind these bitches wikipedians who ke...   \n",
       "9640      256   September 17th vandalized \\n\\nThis date which...   \n",
       "20229     209  Absolutely hysterical.  Go get consensus on th...   \n",
       "195       302  \"\\n\\nRebuild of Evangelion revert warring\\n\\nT...   \n",
       "196       300  \"\\n\\nRebuild of Evangelion revert warring\\n\\nT...   \n",
       "11725     572        User:Adam1213/warn>\\n\\n Vandalism warnings    \n",
       "11724     732        User:Adam1213/warn>\\n\\n Vandalism warnings    \n",
       "10382     489  This user has a history of sock puppetry and l...   \n",
       "10381     171  This user has a history of sock puppetry and l...   \n",
       "10380      22  This user has a history of sock puppetry and l...   \n",
       "\n",
       "                                              more_toxic  correct        p1  \\\n",
       "24702  \"\\n\\nA more important point about Vuia is that...        0  0.126276   \n",
       "9704    Sexism \\n\\nI've tried my hardest to explain t...        0  0.040171   \n",
       "9703    Sexism \\n\\nI've tried my hardest to explain t...        0  0.040171   \n",
       "4571    THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...        0  0.150288   \n",
       "4569    THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...        0  0.150288   \n",
       "4570    THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...        0  0.150288   \n",
       "26873  Thank You your the Wellcome\\nARABIC ASSHOLR GO...        0  0.363390   \n",
       "1934    Hey RGT: you're not just simply disagreeing. ...        0  0.129691   \n",
       "1935    Hey RGT: you're not just simply disagreeing. ...        0  0.129691   \n",
       "1936    Hey RGT: you're not just simply disagreeing. ...        0  0.129691   \n",
       "27670  You are an arrogant little so-and-so - aren't ...        0  0.297834   \n",
       "9640   \"\\n\\nSorry, being a fan of wrestling I would a...        0  0.013912   \n",
       "20229   Beckenham Library user blocked on public comp...        0  0.071487   \n",
       "195    \"\\n\\n A barnstar for you! \\n\\n  The Original B...        0  0.072787   \n",
       "196    \"\\n\\n A barnstar for you! \\n\\n  The Original B...        0  0.072787   \n",
       "11725  A Moldavian\\nI am a Moldavian, and I can expla...        0 -0.009541   \n",
       "11724  A Moldavian\\nI am a Moldavian, and I can expla...        0 -0.009541   \n",
       "10382  \"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...        0  0.062868   \n",
       "10381  \"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...        0  0.062868   \n",
       "10380  \"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...        0  0.062868   \n",
       "\n",
       "             p2      diff  \n",
       "24702  0.126274  0.000002  \n",
       "9704   0.040115  0.000056  \n",
       "9703   0.040115  0.000056  \n",
       "4571   0.150198  0.000090  \n",
       "4569   0.150198  0.000090  \n",
       "4570   0.150198  0.000090  \n",
       "26873  0.363255  0.000136  \n",
       "1934   0.129503  0.000188  \n",
       "1935   0.129503  0.000188  \n",
       "1936   0.129503  0.000188  \n",
       "27670  0.297586  0.000248  \n",
       "9640   0.013654  0.000259  \n",
       "20229  0.071217  0.000269  \n",
       "195    0.072501  0.000286  \n",
       "196    0.072501  0.000286  \n",
       "11725 -0.009871  0.000330  \n",
       "11724 -0.009871  0.000330  \n",
       "10382  0.062414  0.000453  \n",
       "10381  0.062414  0.000453  \n",
       "10380  0.062414  0.000453  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Incorrect predictions with similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>correct</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>246</td>\n",
       "      <td>CaliforniaAliBaba is a Bitch\\nGo fuck yourself...</td>\n",
       "      <td>I've heard a snippet on their myspace page but...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.275426</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>1.277321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28241</th>\n",
       "      <td>118</td>\n",
       "      <td>you are a stupid fuck \\n\\nand your mother's c...</td>\n",
       "      <td>\"\\n\\n  :::   According to Stratfordian authori...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.184728</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>1.134825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>431</td>\n",
       "      <td>ASSHOLES! COME AND TRY ME YOU STUPID FUCK SON ...</td>\n",
       "      <td>\"\\n\\nPlease, Cheesebot is not nonsense. It is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.215474</td>\n",
       "      <td>0.112650</td>\n",
       "      <td>1.102823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19411</th>\n",
       "      <td>46</td>\n",
       "      <td>Ok, what is it \\n\\nWhy are you such a queer.....</td>\n",
       "      <td>wow the guy who deleated the stuff on the Habb...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.161545</td>\n",
       "      <td>0.069165</td>\n",
       "      <td>1.092379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21059</th>\n",
       "      <td>333</td>\n",
       "      <td>fuck you \\n\\nyou're an idiot</td>\n",
       "      <td>What makes you an editor? Why do you use the n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.125096</td>\n",
       "      <td>0.070252</td>\n",
       "      <td>1.054844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18002</th>\n",
       "      <td>634</td>\n",
       "      <td>Are you lost, stupid fucking idiots</td>\n",
       "      <td>Appeal \\n\\n{{unblock|G'day everyone. First, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.124004</td>\n",
       "      <td>0.075780</td>\n",
       "      <td>1.048224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7788</th>\n",
       "      <td>580</td>\n",
       "      <td>suck my dick \\n\\nfuck you dirty whore bitch</td>\n",
       "      <td>Stop being such a sissy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.362443</td>\n",
       "      <td>0.319210</td>\n",
       "      <td>1.043232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11226</th>\n",
       "      <td>320</td>\n",
       "      <td>I fucked your dad last night you fucking whor...</td>\n",
       "      <td>Please don't try and mark a page for deletion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.071216</td>\n",
       "      <td>0.067594</td>\n",
       "      <td>1.003623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191</th>\n",
       "      <td>504</td>\n",
       "      <td>cunt. I hate your face. You are a fucking shit...</td>\n",
       "      <td>\"\\n\\nAll my uploaded pdf-documents have been u...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.152207</td>\n",
       "      <td>0.158813</td>\n",
       "      <td>0.993394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25754</th>\n",
       "      <td>190</td>\n",
       "      <td>what the hell man? why did you delete my page ...</td>\n",
       "      <td>\" December 2009 (UTC)\\n\\n Now there's typical ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.186038</td>\n",
       "      <td>0.199026</td>\n",
       "      <td>0.987012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20435</th>\n",
       "      <td>504</td>\n",
       "      <td>Fuck you niggerkite. y you deleting so much sh...</td>\n",
       "      <td>Bush is the man. \\n\\nHe had the balls to disp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.117708</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.985845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>733</td>\n",
       "      <td>Steve Little \\n\\nSteve Little's character in ...</td>\n",
       "      <td>The only thing I can considor I an attack was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985752</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.983224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9175</th>\n",
       "      <td>439</td>\n",
       "      <td>\\nPENIS COCKWAD NIGGA NIGGA NYUGGGGAAA!!! NIG...</td>\n",
       "      <td>\"\\n\\nConstance Cumbey Update on March 5, 2005 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.015603</td>\n",
       "      <td>0.039863</td>\n",
       "      <td>0.975740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17193</th>\n",
       "      <td>238</td>\n",
       "      <td>fuck this this is bull shit and it does not ev...</td>\n",
       "      <td>Just To Let You Know. You have no life.\\nI hat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>0.213855</td>\n",
       "      <td>0.972487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>347</td>\n",
       "      <td>fuck this this is bull shit and it does not ev...</td>\n",
       "      <td>Just To Let You Know. You have no life.\\nI hat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>0.213855</td>\n",
       "      <td>0.972487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9227</th>\n",
       "      <td>78</td>\n",
       "      <td>Yea well kiss my fucker shit nuts!</td>\n",
       "      <td>Azari or Azerbaijani? \\n\\nAzari-iranian,azerb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.939252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29057</th>\n",
       "      <td>716</td>\n",
       "      <td>Fuck You\\n\\nGo suck on a fat dick faggot mothe...</td>\n",
       "      <td>\"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.270009</td>\n",
       "      <td>0.338313</td>\n",
       "      <td>0.931695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29056</th>\n",
       "      <td>204</td>\n",
       "      <td>Fuck You\\n\\nGo suck on a fat dick faggot mothe...</td>\n",
       "      <td>\"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.270009</td>\n",
       "      <td>0.338313</td>\n",
       "      <td>0.931695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22221</th>\n",
       "      <td>441</td>\n",
       "      <td>Green Tea and CigarettesWhy you're a worthles...</td>\n",
       "      <td>It doesn't offend me, but that's the thing abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.178067</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>0.931257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22222</th>\n",
       "      <td>254</td>\n",
       "      <td>Green Tea and CigarettesWhy you're a worthles...</td>\n",
       "      <td>It doesn't offend me, but that's the thing abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.178067</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>0.931257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "25062     246  CaliforniaAliBaba is a Bitch\\nGo fuck yourself...   \n",
       "28241     118   you are a stupid fuck \\n\\nand your mother's c...   \n",
       "1952      431  ASSHOLES! COME AND TRY ME YOU STUPID FUCK SON ...   \n",
       "19411      46   Ok, what is it \\n\\nWhy are you such a queer.....   \n",
       "21059     333                       fuck you \\n\\nyou're an idiot   \n",
       "18002     634                Are you lost, stupid fucking idiots   \n",
       "7788      580        suck my dick \\n\\nfuck you dirty whore bitch   \n",
       "11226     320   I fucked your dad last night you fucking whor...   \n",
       "20191     504  cunt. I hate your face. You are a fucking shit...   \n",
       "25754     190  what the hell man? why did you delete my page ...   \n",
       "20435     504  Fuck you niggerkite. y you deleting so much sh...   \n",
       "12482     733   Steve Little \\n\\nSteve Little's character in ...   \n",
       "9175      439   \\nPENIS COCKWAD NIGGA NIGGA NYUGGGGAAA!!! NIG...   \n",
       "17193     238  fuck this this is bull shit and it does not ev...   \n",
       "17192     347  fuck this this is bull shit and it does not ev...   \n",
       "9227       78                 Yea well kiss my fucker shit nuts!   \n",
       "29057     716  Fuck You\\n\\nGo suck on a fat dick faggot mothe...   \n",
       "29056     204  Fuck You\\n\\nGo suck on a fat dick faggot mothe...   \n",
       "22221     441   Green Tea and CigarettesWhy you're a worthles...   \n",
       "22222     254   Green Tea and CigarettesWhy you're a worthles...   \n",
       "\n",
       "                                              more_toxic  correct        p1  \\\n",
       "25062  I've heard a snippet on their myspace page but...        0  1.275426   \n",
       "28241  \"\\n\\n  :::   According to Stratfordian authori...        0  1.184728   \n",
       "1952   \"\\n\\nPlease, Cheesebot is not nonsense. It is ...        0  1.215474   \n",
       "19411  wow the guy who deleated the stuff on the Habb...        0  1.161545   \n",
       "21059  What makes you an editor? Why do you use the n...        0  1.125096   \n",
       "18002   Appeal \\n\\n{{unblock|G'day everyone. First, I...        0  1.124004   \n",
       "7788                           Stop being such a sissy.         0  1.362443   \n",
       "11226  Please don't try and mark a page for deletion ...        0  1.071216   \n",
       "20191  \"\\n\\nAll my uploaded pdf-documents have been u...        0  1.152207   \n",
       "25754  \" December 2009 (UTC)\\n\\n Now there's typical ...        0  1.186038   \n",
       "20435   Bush is the man. \\n\\nHe had the balls to disp...        0  1.117708   \n",
       "12482   The only thing I can considor I an attack was...        0  0.985752   \n",
       "9175   \"\\n\\nConstance Cumbey Update on March 5, 2005 ...        0  1.015603   \n",
       "17193  Just To Let You Know. You have no life.\\nI hat...        0  1.186342   \n",
       "17192  Just To Let You Know. You have no life.\\nI hat...        0  1.186342   \n",
       "9227    Azari or Azerbaijani? \\n\\nAzari-iranian,azerb...        0  0.943498   \n",
       "29057  \"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...        0  1.270009   \n",
       "29056  \"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...        0  1.270009   \n",
       "22221  It doesn't offend me, but that's the thing abo...        0  1.178067   \n",
       "22222  It doesn't offend me, but that's the thing abo...        0  1.178067   \n",
       "\n",
       "             p2      diff  \n",
       "25062 -0.001895  1.277321  \n",
       "28241  0.049904  1.134825  \n",
       "1952   0.112650  1.102823  \n",
       "19411  0.069165  1.092379  \n",
       "21059  0.070252  1.054844  \n",
       "18002  0.075780  1.048224  \n",
       "7788   0.319210  1.043232  \n",
       "11226  0.067594  1.003623  \n",
       "20191  0.158813  0.993394  \n",
       "25754  0.199026  0.987012  \n",
       "20435  0.131863  0.985845  \n",
       "12482  0.002528  0.983224  \n",
       "9175   0.039863  0.975740  \n",
       "17193  0.213855  0.972487  \n",
       "17192  0.213855  0.972487  \n",
       "9227   0.004246  0.939252  \n",
       "29057  0.338313  0.931695  \n",
       "29056  0.338313  0.931695  \n",
       "22221  0.246810  0.931257  \n",
       "22222  0.246810  0.931257  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Incorrect predictions with dis-similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using pipeline\n",
    "df_sub['score'] = test_preds_arr.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cases with duplicates scores\n",
    "df_sub['score'].count() - df_sub['score'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572927</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464975</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303382</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022798</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230748</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.130651</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.064500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.124117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.230264</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145350</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  score\n",
       "0  0.572927      2\n",
       "1  0.464975      2\n",
       "2  0.303382      2\n",
       "3  0.022798      2\n",
       "4  0.230748      2\n",
       "5  0.130651      2\n",
       "6  0.064500      2\n",
       "7  0.124117      2\n",
       "8  0.230264      2\n",
       "9  0.145350      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_score = df_sub['score'].value_counts().reset_index()[:10]\n",
    "same_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>95080362</td>\n",
       "      <td>\"\\n\\nPlease do not add nonsense to Wikipedia. ...</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>160935265</td>\n",
       "      <td>\"\\n\\nPlease do not add nonsense to Wikipedia. ...</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>275797183</td>\n",
       "      <td>Hi\\n\\nCould you please learn to interact like ...</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>275812977</td>\n",
       "      <td>Could you please learn to interact like a sent...</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>298854514</td>\n",
       "      <td>her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...</td>\n",
       "      <td>0.464975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>301925517</td>\n",
       "      <td>her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...</td>\n",
       "      <td>0.464975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>339478276</td>\n",
       "      <td>I'm gonna beat you to a bloody pulp then sho...</td>\n",
       "      <td>0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>339478966</td>\n",
       "      <td>I'm gonna beat you to a bloody pulp then shoo...</td>\n",
       "      <td>0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>345043812</td>\n",
       "      <td>JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A MO...</td>\n",
       "      <td>0.303382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>345043888</td>\n",
       "      <td>JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A M...</td>\n",
       "      <td>0.303382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>346641598</td>\n",
       "      <td>WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>346641762</td>\n",
       "      <td>WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>375083006</td>\n",
       "      <td>FC*K U\\n\\nWhy the fc*k should I get a warning ...</td>\n",
       "      <td>0.145350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>375157867</td>\n",
       "      <td>FC*K U\\n\\nWhy the fc*k should I get a warning ...</td>\n",
       "      <td>0.145350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>444588772</td>\n",
       "      <td>i will ki \\n\\nll you and wear your skin like ...</td>\n",
       "      <td>0.130651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>444589429</td>\n",
       "      <td>I will ki \\n\\nll you and wear your skin like ...</td>\n",
       "      <td>0.130651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>481969878</td>\n",
       "      <td>I will flay you alive, you fking stalker.  \\n...</td>\n",
       "      <td>0.230264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>481970432</td>\n",
       "      <td>I will flay you alive, you fking stalker. \\n\\...</td>\n",
       "      <td>0.230264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>501720037</td>\n",
       "      <td>Eat shit nigger \\n\\nI have infinite Ips I can...</td>\n",
       "      <td>0.572927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>501720124</td>\n",
       "      <td>Eat shit nigger \\n\\nI cant be blocked I have ...</td>\n",
       "      <td>0.572927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text     score\n",
       "1832    95080362  \"\\n\\nPlease do not add nonsense to Wikipedia. ...  0.022798\n",
       "2842   160935265  \"\\n\\nPlease do not add nonsense to Wikipedia. ...  0.022798\n",
       "4832   275797183  Hi\\n\\nCould you please learn to interact like ...  0.064500\n",
       "4833   275812977  Could you please learn to interact like a sent...  0.064500\n",
       "5140   298854514  her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...  0.464975\n",
       "5190   301925517  her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...  0.464975\n",
       "5752   339478276    I'm gonna beat you to a bloody pulp then sho...  0.230748\n",
       "5753   339478966   I'm gonna beat you to a bloody pulp then shoo...  0.230748\n",
       "5832   345043812  JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A MO...  0.303382\n",
       "5833   345043888   JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A M...  0.303382\n",
       "5852   346641598  WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...  0.124117\n",
       "5853   346641762  WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...  0.124117\n",
       "6193   375083006  FC*K U\\n\\nWhy the fc*k should I get a warning ...  0.145350\n",
       "6194   375157867  FC*K U\\n\\nWhy the fc*k should I get a warning ...  0.145350\n",
       "6947   444588772   i will ki \\n\\nll you and wear your skin like ...  0.130651\n",
       "6948   444589429   I will ki \\n\\nll you and wear your skin like ...  0.130651\n",
       "7312   481969878   I will flay you alive, you fking stalker.  \\n...  0.230264\n",
       "7313   481970432   I will flay you alive, you fking stalker. \\n\\...  0.230264\n",
       "7503   501720037   Eat shit nigger \\n\\nI have infinite Ips I can...  0.572927\n",
       "7504   501720124   Eat shit nigger \\n\\nI cant be blocked I have ...  0.572927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub['score'].isin(same_score['index'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>186197494</td>\n",
       "      <td>\"\\nFor copying and pasting of what I felt stro...</td>\n",
       "      <td>0.141626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>116257386</td>\n",
       "      <td>Dude!  \\nThat was an attempt at saying somethi...</td>\n",
       "      <td>0.160497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7070</th>\n",
       "      <td>457417171</td>\n",
       "      <td>You simply display your ignorance.  Fatuorum</td>\n",
       "      <td>0.274591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>242591983</td>\n",
       "      <td>\"\\n\\nSockpuppetry case\\n \\nYou have been accus...</td>\n",
       "      <td>0.012607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>70880071</td>\n",
       "      <td>Now let's see who's gonna start crying like a ...</td>\n",
       "      <td>0.279953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text     score\n",
       "3294   186197494  \"\\nFor copying and pasting of what I felt stro...  0.141626\n",
       "2167   116257386  Dude!  \\nThat was an attempt at saying somethi...  0.160497\n",
       "7070   457417171      You simply display your ignorance.  Fatuorum   0.274591\n",
       "4347   242591983  \"\\n\\nSockpuppetry case\\n \\nYou have been accus...  0.012607\n",
       "1370    70880071  Now let's see who's gonna start crying like a ...  0.279953"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission\n",
    "df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c9a7b156ba7ef82939fe85a8bbe37517a5f7091dd1ebbff4ae1cdb2f418c7af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('venv_py38': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
