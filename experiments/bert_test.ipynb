{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import getData, clean\n",
    "from data import DataProcessor\n",
    "from model import AutoRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cofig\n",
    "CONFIG = dict(\n",
    "    seed = 12345,\n",
    "    pretrained_model = 'bert-base-uncased',\n",
    "    output_dir = '../models/bert_regression_mini',\n",
    "    train_file = '4th/v0/train.csv',\n",
    "    dev_file = '4th/v0/dev.csv',\n",
    "    train_batch_size = 32,\n",
    "    dev_batch_size = 32,\n",
    "    lr = 5e-5,\n",
    "    epochs = 5,\n",
    "    num_class = 1,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    device_ids = [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'BertRegressor' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b73b475d6ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tokenizer.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pretrained_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bert_regressor.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'BertRegressor' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# # download transformers pretrained model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(CONFIG['pretrained_model'])\n",
    "# tokenizer.save_pretrained(os.path.join(CONFIG['output_dir'], 'tokenizer.pt'))\n",
    "# bert = AutoModel.from_pretrained(CONFIG['pretrained_model'])\n",
    "# bert.save_pretrained(os.path.join(CONFIG['output_dir'], 'bert.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# init bert pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['pretrained_model'])\n",
    "model = AutoRegressor(CONFIG['pretrained_model'], CONFIG['num_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4th/v0/train.csv ...\n",
      "Read 4th/v0/dev.csv ...\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# train dataset\n",
    "train_df = getData(data_path=CONFIG['train_file'])\n",
    "# data processing with tokenizing\n",
    "train_data = DataProcessor(train_df, tokenizer, is_eval=False)\n",
    "train_dataloader = DataLoader(train_data, batch_size=CONFIG['train_batch_size'], shuffle=True, num_workers=4)\n",
    "\n",
    "# dev dataset\n",
    "dev_df = getData(data_path=CONFIG['dev_file'])\n",
    "# data processing with tokenizing\n",
    "dev_data = DataProcessor(dev_df, tokenizer, is_eval=False)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=CONFIG['dev_batch_size'], shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train or Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataloader, dev_dataloader, criterion, optimizer, scheduler, device, output_dir):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_loss_train = 0.0\n",
    "\n",
    "            print(f\"[Epochs : {epoch_num+1}/{epochs}]\")\n",
    "            for i, (train_input, train_label) in enumerate(tqdm(train_dataloader)):\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                output = torch.squeeze(output, 1)\n",
    "                del input_id\n",
    "                del mask\n",
    "                \n",
    "                train_label = train_label.to(device)\n",
    "                batch_loss = criterion(output.float(), train_label.float())\n",
    "                del train_label\n",
    "\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                if i%10000 == 0:  \n",
    "                    print(f'Epochs: {epoch_num + 1} | Train Loss: {batch_loss: .3f}')\n",
    "                    torch.save(model.state_dict(),\\\n",
    "                            os.path.join(output_dir, f'bert_regression-{epoch_num+1}-{i}.pt'))\n",
    "\n",
    "            # validate using our dev set \n",
    "            model.eval()\n",
    "            total_loss_dev = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for dev_input, dev_label in dev_dataloader:\n",
    "                    dev_label = dev_label.to(device)\n",
    "                    input_id = dev_input['input_ids'].squeeze(1).to(device)\n",
    "                    mask = dev_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "                    output = torch.squeeze(output, 1)\n",
    "\n",
    "                    batch_loss = criterion(output.float(), dev_label.float())\n",
    "                    total_loss_dev += batch_loss.item()\n",
    "\n",
    "                    del dev_label\n",
    "                    del input_id\n",
    "                    del mask\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f} \\\n",
    "                | Val Loss: {total_loss_dev / len(dev_dataloader): .3f}')\n",
    "\n",
    "            torch.save(model.state_dict(),\\\n",
    "                    os.path.join(output_dir, f'bert_regression-{epoch_num+1}-{len(train_dataloader)}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertRegressor(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (regressor): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(CONFIG['output_dir'], 'model_ckpt.pt'))\n",
    "# checkpoint = torch.load(os.path.join(CONFIG['output_dir'], 'model_ckpt-55587.pt'))\n",
    "model.load_state_dict(checkpoint)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model, device_ids=CONFIG['device_ids'])\n",
    "model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4th/validation_cleaned.csv ...\n",
      "Read 4th/comments_to_score.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Validation data \n",
    "df_val = getData(data_path=\"4th/validation_cleaned.csv\")\n",
    "# Test data\n",
    "df_sub = getData(data_path=\"4th/comments_to_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n",
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n",
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(r'([*!?\\']+)',r' \\1 ')\n",
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:56: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n",
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:57: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n",
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:58: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(r'[ ]{2,}|\\n',' ')\n",
      "/home/super/study/jigsaw-toxic-competition/experiments/utils.py:70: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data = data.str.replace(pattern, '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        Gjalexei, you asked about whether there is...\n",
       "1    Looks like be have an abuser , can you please ...\n",
       "2    I confess to having complete (and apparently b...\n",
       "3      Freud ' s ideas are certainly much discussed...\n",
       "4    It is not just you. This is a laundry list of ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = clean(df_sub, 'text')\n",
    "df_sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1_data = DataProcessor(df_val['less_toxic'], tokenizer, is_eval=True)\n",
    "val1_dataloader = DataLoader(val1_data, batch_size=CONFIG['dev_batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "val2_data = DataProcessor(df_val['more_toxic'], tokenizer, is_eval=True)\n",
    "val2_dataloader = DataLoader(val2_data, batch_size=CONFIG['dev_batch_size'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_dataloader, device):\n",
    "\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tmodel.eval()\n",
    "\n",
    "\toutputs = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, val_input in enumerate(tqdm(val_dataloader)):\n",
    "\t\t\tinput_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\t\t\tmask = val_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "\t\t\toutput = model(input_id, mask)\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tprint(output.shape)\n",
    "\t\t\t#output = torch.squeeze(output, 1)\n",
    "\t\t\toutputs.extend(output.detach().cpu().numpy())\n",
    "\n",
    "\t\t\tdel input_id\n",
    "\t\t\tdel mask\n",
    "\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation\n",
    "- final validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict less toxic\n",
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 941/941 [02:47<00:00,  5.63it/s]\n",
      "  0%|          | 0/941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict more toxic\n",
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 941/941 [02:48<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "device = CONFIG['device']\n",
    "print('predict less toxic')\n",
    "p1 = predict(model, val1_dataloader, device)\t\n",
    "print('predict more toxic')\n",
    "p2 = predict(model, val2_dataloader, device)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.asarray(p1)\n",
    "p2 = np.asarray(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy is 66.23\n"
     ]
    }
   ],
   "source": [
    "print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['p1'] = p1\n",
    "df_val['p2'] = p2\n",
    "df_val['diff'] = np.abs(p2 - p1)\n",
    "df_val['correct'] = (p1 < p2).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22995</th>\n",
       "      <td>554</td>\n",
       "      <td>Good move on your part by listing the Billy R...</td>\n",
       "      <td>User:Lagoo Sab is a sockpuppet of User:Ketabt...</td>\n",
       "      <td>0.094230</td>\n",
       "      <td>0.094181</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22996</th>\n",
       "      <td>316</td>\n",
       "      <td>Good move on your part by listing the Billy R...</td>\n",
       "      <td>User:Lagoo Sab is a sockpuppet of User:Ketabt...</td>\n",
       "      <td>0.094230</td>\n",
       "      <td>0.094181</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14357</th>\n",
       "      <td>302</td>\n",
       "      <td>ASK YOUR MOTHER &lt;&lt;&lt;&gt;&gt;</td>\n",
       "      <td>Bisexual pride flag   the bisexual pride flag...</td>\n",
       "      <td>0.091989</td>\n",
       "      <td>0.091826</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14356</th>\n",
       "      <td>208</td>\n",
       "      <td>ASK YOUR MOTHER &lt;&lt;&lt;&gt;&gt;</td>\n",
       "      <td>Bisexual pride flag   the bisexual pride flag...</td>\n",
       "      <td>0.091989</td>\n",
       "      <td>0.091826</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14355</th>\n",
       "      <td>393</td>\n",
       "      <td>ASK YOUR MOTHER &lt;&lt;&lt;&gt;&gt;</td>\n",
       "      <td>Bisexual pride flag   the bisexual pride flag...</td>\n",
       "      <td>0.091989</td>\n",
       "      <td>0.091826</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21725</th>\n",
       "      <td>675</td>\n",
       "      <td>. (Ah, did I hurt you ? ) Just call this a res...</td>\n",
       "      <td>YOUR ZEALOUS CONTRIBUTIONS TO THE DAHN YOGA ...</td>\n",
       "      <td>0.101272</td>\n",
       "      <td>0.101091</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21724</th>\n",
       "      <td>74</td>\n",
       "      <td>. (Ah, did I hurt you ? ) Just call this a res...</td>\n",
       "      <td>YOUR ZEALOUS CONTRIBUTIONS TO THE DAHN YOGA ...</td>\n",
       "      <td>0.101272</td>\n",
       "      <td>0.101091</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21723</th>\n",
       "      <td>417</td>\n",
       "      <td>. (Ah, did I hurt you ? ) Just call this a res...</td>\n",
       "      <td>YOUR ZEALOUS CONTRIBUTIONS TO THE DAHN YOGA ...</td>\n",
       "      <td>0.101272</td>\n",
       "      <td>0.101091</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td>485</td>\n",
       "      <td>Tens of thousands of innocent Muslims ? Jaysu...</td>\n",
       "      <td>ps   i love to destroy. i guess creating is g...</td>\n",
       "      <td>0.205743</td>\n",
       "      <td>0.205559</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>74</td>\n",
       "      <td>im going to punch jim wales one day. ill find ...</td>\n",
       "      <td>hiding behind these bitches wikipedians who ke...</td>\n",
       "      <td>0.502962</td>\n",
       "      <td>0.502755</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>481</td>\n",
       "      <td>P . S- How the hell are you going to block me ...</td>\n",
       "      <td>eekster   hello eekster you gya azz ho reply ...</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>0.166986</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>726</td>\n",
       "      <td>P . S- How the hell are you going to block me ...</td>\n",
       "      <td>eekster   hello eekster you gya azz ho reply ...</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>0.166986</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29839</th>\n",
       "      <td>254</td>\n",
       "      <td>What ' s the matter Haworth, affraid I ' ll ...</td>\n",
       "      <td>Let the fight go on   Have U no decency you h...</td>\n",
       "      <td>0.370254</td>\n",
       "      <td>0.369965</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29838</th>\n",
       "      <td>308</td>\n",
       "      <td>What ' s the matter Haworth, affraid I ' ll ...</td>\n",
       "      <td>Let the fight go on   Have U no decency you h...</td>\n",
       "      <td>0.370254</td>\n",
       "      <td>0.369965</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16720</th>\n",
       "      <td>112</td>\n",
       "      <td>you are a cunt   so how about a blowjob ?</td>\n",
       "      <td>Go fuck yourself you faggot -</td>\n",
       "      <td>0.629908</td>\n",
       "      <td>0.629482</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16719</th>\n",
       "      <td>265</td>\n",
       "      <td>you are a cunt   so how about a blowjob ?</td>\n",
       "      <td>Go fuck yourself you faggot -</td>\n",
       "      <td>0.629908</td>\n",
       "      <td>0.629482</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7256</th>\n",
       "      <td>730</td>\n",
       "      <td>Welcome. I think this might officially make yo...</td>\n",
       "      <td>Careful friend, I was censored for telling som...</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.302185</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7255</th>\n",
       "      <td>562</td>\n",
       "      <td>Welcome. I think this might officially make yo...</td>\n",
       "      <td>Careful friend, I was censored for telling som...</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.302185</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18340</th>\n",
       "      <td>508</td>\n",
       "      <td>On musical influences section ==  Musical infl...</td>\n",
       "      <td>............. p u suck</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18339</th>\n",
       "      <td>679</td>\n",
       "      <td>On musical influences section ==  Musical infl...</td>\n",
       "      <td>............. p u suck</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "22995     554   Good move on your part by listing the Billy R...   \n",
       "22996     316   Good move on your part by listing the Billy R...   \n",
       "14357     302                            ASK YOUR MOTHER <<<>>     \n",
       "14356     208                            ASK YOUR MOTHER <<<>>     \n",
       "14355     393                            ASK YOUR MOTHER <<<>>     \n",
       "21725     675  . (Ah, did I hurt you ? ) Just call this a res...   \n",
       "21724      74  . (Ah, did I hurt you ? ) Just call this a res...   \n",
       "21723     417  . (Ah, did I hurt you ? ) Just call this a res...   \n",
       "8057      485   Tens of thousands of innocent Muslims ? Jaysu...   \n",
       "5265       74  im going to punch jim wales one day. ill find ...   \n",
       "10998     481  P . S- How the hell are you going to block me ...   \n",
       "10997     726  P . S- How the hell are you going to block me ...   \n",
       "29839     254    What ' s the matter Haworth, affraid I ' ll ...   \n",
       "29838     308    What ' s the matter Haworth, affraid I ' ll ...   \n",
       "16720     112         you are a cunt   so how about a blowjob ?    \n",
       "16719     265         you are a cunt   so how about a blowjob ?    \n",
       "7256      730  Welcome. I think this might officially make yo...   \n",
       "7255      562  Welcome. I think this might officially make yo...   \n",
       "18340     508  On musical influences section ==  Musical infl...   \n",
       "18339     679  On musical influences section ==  Musical infl...   \n",
       "\n",
       "                                              more_toxic        p1        p2  \\\n",
       "22995   User:Lagoo Sab is a sockpuppet of User:Ketabt...  0.094230  0.094181   \n",
       "22996   User:Lagoo Sab is a sockpuppet of User:Ketabt...  0.094230  0.094181   \n",
       "14357   Bisexual pride flag   the bisexual pride flag...  0.091989  0.091826   \n",
       "14356   Bisexual pride flag   the bisexual pride flag...  0.091989  0.091826   \n",
       "14355   Bisexual pride flag   the bisexual pride flag...  0.091989  0.091826   \n",
       "21725    YOUR ZEALOUS CONTRIBUTIONS TO THE DAHN YOGA ...  0.101272  0.101091   \n",
       "21724    YOUR ZEALOUS CONTRIBUTIONS TO THE DAHN YOGA ...  0.101272  0.101091   \n",
       "21723    YOUR ZEALOUS CONTRIBUTIONS TO THE DAHN YOGA ...  0.101272  0.101091   \n",
       "8057    ps   i love to destroy. i guess creating is g...  0.205743  0.205559   \n",
       "5265   hiding behind these bitches wikipedians who ke...  0.502962  0.502755   \n",
       "10998   eekster   hello eekster you gya azz ho reply ...  0.167234  0.166986   \n",
       "10997   eekster   hello eekster you gya azz ho reply ...  0.167234  0.166986   \n",
       "29839   Let the fight go on   Have U no decency you h...  0.370254  0.369965   \n",
       "29838   Let the fight go on   Have U no decency you h...  0.370254  0.369965   \n",
       "16720                     Go fuck yourself you faggot -   0.629908  0.629482   \n",
       "16719                     Go fuck yourself you faggot -   0.629908  0.629482   \n",
       "7256   Careful friend, I was censored for telling som...  0.302646  0.302185   \n",
       "7255   Careful friend, I was censored for telling som...  0.302646  0.302185   \n",
       "18340                             ............. p u suck  0.151751  0.151135   \n",
       "18339                             ............. p u suck  0.151751  0.151135   \n",
       "\n",
       "           diff  correct  \n",
       "22995  0.000049        0  \n",
       "22996  0.000049        0  \n",
       "14357  0.000164        0  \n",
       "14356  0.000164        0  \n",
       "14355  0.000164        0  \n",
       "21725  0.000181        0  \n",
       "21724  0.000181        0  \n",
       "21723  0.000181        0  \n",
       "8057   0.000184        0  \n",
       "5265   0.000207        0  \n",
       "10998  0.000248        0  \n",
       "10997  0.000248        0  \n",
       "29839  0.000290        0  \n",
       "29838  0.000290        0  \n",
       "16720  0.000426        0  \n",
       "16719  0.000426        0  \n",
       "7256   0.000461        0  \n",
       "7255   0.000461        0  \n",
       "18340  0.000616        0  \n",
       "18339  0.000616        0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Incorrect predictions with similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>246</td>\n",
       "      <td>CaliforniaAliBaba is a Bitch Go fuck yourself ...</td>\n",
       "      <td>I ' ve heard a snippet on their myspace page b...</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28241</th>\n",
       "      <td>118</td>\n",
       "      <td>you are a stupid fuck   and your mother ' s c...</td>\n",
       "      <td>::: According to Stratfordian authority H ....</td>\n",
       "      <td>0.646331</td>\n",
       "      <td>0.014239</td>\n",
       "      <td>0.632091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9800</th>\n",
       "      <td>488</td>\n",
       "      <td>Move it to just Milwaukee, you FAT JACKASS !!!</td>\n",
       "      <td>Vandalism  Please refrain from adding nonsense...</td>\n",
       "      <td>0.640709</td>\n",
       "      <td>0.014517</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13424</th>\n",
       "      <td>633</td>\n",
       "      <td>Everybody lock ur dicks cuz theyre gay !!!</td>\n",
       "      <td>Useless This page is not helpful to Wikipedi...</td>\n",
       "      <td>0.610338</td>\n",
       "      <td>-0.010260</td>\n",
       "      <td>0.620598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>338</td>\n",
       "      <td>Have I made myself clear yet ? It should be p...</td>\n",
       "      <td>Please stop adding nonsense to Wikipedia. It ...</td>\n",
       "      <td>0.620994</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.614451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936</th>\n",
       "      <td>146</td>\n",
       "      <td>ya EXTRA   Ur fuckin EXTRA tryna bryte up urs...</td>\n",
       "      <td>::::::Whatever, my policy is that I think that...</td>\n",
       "      <td>0.613233</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.613466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14301</th>\n",
       "      <td>193</td>\n",
       "      <td>Warnings   you asshole gave me mutiple warnin...</td>\n",
       "      <td>Editor issue I noticed you have had a bit of a...</td>\n",
       "      <td>0.600950</td>\n",
       "      <td>-0.012351</td>\n",
       "      <td>0.613301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>216</td>\n",
       "      <td>Hey fuck you cunt bag ! ,you dont own wikpedia...</td>\n",
       "      <td>Please stop your disruptive editing. If you...</td>\n",
       "      <td>0.643886</td>\n",
       "      <td>0.031957</td>\n",
       "      <td>0.611929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>639</td>\n",
       "      <td>Hey fuck you cunt bag ! ,you dont own wikpedia...</td>\n",
       "      <td>Please stop your disruptive editing. If you...</td>\n",
       "      <td>0.643886</td>\n",
       "      <td>0.031957</td>\n",
       "      <td>0.611929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13641</th>\n",
       "      <td>439</td>\n",
       "      <td>You can suck my cock, motherfucker   ALSO, NI...</td>\n",
       "      <td>Speedy deletion of Allan kamuyu   A page you...</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>0.019654</td>\n",
       "      <td>0.611702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15320</th>\n",
       "      <td>118</td>\n",
       "      <td>Dude you ' re gay, now seriously STFU and get ...</td>\n",
       "      <td>vandalism Hi Spute. I began trying to police t...</td>\n",
       "      <td>0.639228</td>\n",
       "      <td>0.027979</td>\n",
       "      <td>0.611249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8288</th>\n",
       "      <td>742</td>\n",
       "      <td>I ' M F *** ING SERIOUS !   MY COUSIN ' S C...</td>\n",
       "      <td>What are you talking about ? I don ' t even ...</td>\n",
       "      <td>0.634868</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>0.610032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3232</th>\n",
       "      <td>591</td>\n",
       "      <td>STOP POSTING WARNIGN ON MY TALK PAGE ITS FUCKI...</td>\n",
       "      <td>(UTC)  Ass hole - 17:53, 9 March 2007</td>\n",
       "      <td>0.598918</td>\n",
       "      <td>-0.011087</td>\n",
       "      <td>0.610006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>530</td>\n",
       "      <td>LOL; charles, you are a real fag aren ' t you ...</td>\n",
       "      <td>I ' m not the one who added that source, nei...</td>\n",
       "      <td>0.617908</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.609843</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>484</td>\n",
       "      <td>LOL; charles, you are a real fag aren ' t you ...</td>\n",
       "      <td>I ' m not the one who added that source, nei...</td>\n",
       "      <td>0.617908</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.609843</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25413</th>\n",
       "      <td>515</td>\n",
       "      <td>hi my name is monkeyman and i am a pedo and i ...</td>\n",
       "      <td>) I. Oh the encyclopaedia that anyone can edi...</td>\n",
       "      <td>0.667642</td>\n",
       "      <td>0.058770</td>\n",
       "      <td>0.608871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26060</th>\n",
       "      <td>78</td>\n",
       "      <td>That ' s becasue you are an asshole and a cunt...</td>\n",
       "      <td>Anti-Catholicism   Your edits are highly spur...</td>\n",
       "      <td>0.632122</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>0.607676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26059</th>\n",
       "      <td>110</td>\n",
       "      <td>That ' s becasue you are an asshole and a cunt...</td>\n",
       "      <td>Anti-Catholicism   Your edits are highly spur...</td>\n",
       "      <td>0.632122</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>0.607676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>33</td>\n",
       "      <td>WOW, any thing you don ' t like is childish.....</td>\n",
       "      <td>you ' ve misconstrued an open arguement as a ...</td>\n",
       "      <td>0.618349</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>0.607619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>0</td>\n",
       "      <td>WOW, any thing you don ' t like is childish.....</td>\n",
       "      <td>you ' ve misconstrued an open arguement as a ...</td>\n",
       "      <td>0.618349</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>0.607619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "25062     246  CaliforniaAliBaba is a Bitch Go fuck yourself ...   \n",
       "28241     118   you are a stupid fuck   and your mother ' s c...   \n",
       "9800      488    Move it to just Milwaukee, you FAT JACKASS !!!    \n",
       "13424     633        Everybody lock ur dicks cuz theyre gay !!!    \n",
       "2823      338   Have I made myself clear yet ? It should be p...   \n",
       "17936     146   ya EXTRA   Ur fuckin EXTRA tryna bryte up urs...   \n",
       "14301     193   Warnings   you asshole gave me mutiple warnin...   \n",
       "3331      216  Hey fuck you cunt bag ! ,you dont own wikpedia...   \n",
       "3330      639  Hey fuck you cunt bag ! ,you dont own wikpedia...   \n",
       "13641     439   You can suck my cock, motherfucker   ALSO, NI...   \n",
       "15320     118  Dude you ' re gay, now seriously STFU and get ...   \n",
       "8288      742     I ' M F *** ING SERIOUS !   MY COUSIN ' S C...   \n",
       "3232      591  STOP POSTING WARNIGN ON MY TALK PAGE ITS FUCKI...   \n",
       "566       530  LOL; charles, you are a real fag aren ' t you ...   \n",
       "565       484  LOL; charles, you are a real fag aren ' t you ...   \n",
       "25413     515  hi my name is monkeyman and i am a pedo and i ...   \n",
       "26060      78  That ' s becasue you are an asshole and a cunt...   \n",
       "26059     110  That ' s becasue you are an asshole and a cunt...   \n",
       "5829       33   WOW, any thing you don ' t like is childish.....   \n",
       "5828        0   WOW, any thing you don ' t like is childish.....   \n",
       "\n",
       "                                              more_toxic        p1        p2  \\\n",
       "25062  I ' ve heard a snippet on their myspace page b...  0.655871  0.016380   \n",
       "28241     ::: According to Stratfordian authority H ....  0.646331  0.014239   \n",
       "9800   Vandalism  Please refrain from adding nonsense...  0.640709  0.014517   \n",
       "13424    Useless This page is not helpful to Wikipedi...  0.610338 -0.010260   \n",
       "2823    Please stop adding nonsense to Wikipedia. It ...  0.620994  0.006543   \n",
       "17936  ::::::Whatever, my policy is that I think that...  0.613233 -0.000233   \n",
       "14301  Editor issue I noticed you have had a bit of a...  0.600950 -0.012351   \n",
       "3331      Please stop your disruptive editing. If you...  0.643886  0.031957   \n",
       "3330      Please stop your disruptive editing. If you...  0.643886  0.031957   \n",
       "13641    Speedy deletion of Allan kamuyu   A page you...  0.631355  0.019654   \n",
       "15320  vandalism Hi Spute. I began trying to police t...  0.639228  0.027979   \n",
       "8288     What are you talking about ? I don ' t even ...  0.634868  0.024836   \n",
       "3232               (UTC)  Ass hole - 17:53, 9 March 2007  0.598918 -0.011087   \n",
       "566      I ' m not the one who added that source, nei...  0.617908  0.008065   \n",
       "565      I ' m not the one who added that source, nei...  0.617908  0.008065   \n",
       "25413   ) I. Oh the encyclopaedia that anyone can edi...  0.667642  0.058770   \n",
       "26060   Anti-Catholicism   Your edits are highly spur...  0.632122  0.024445   \n",
       "26059   Anti-Catholicism   Your edits are highly spur...  0.632122  0.024445   \n",
       "5829    you ' ve misconstrued an open arguement as a ...  0.618349  0.010730   \n",
       "5828    you ' ve misconstrued an open arguement as a ...  0.618349  0.010730   \n",
       "\n",
       "           diff  correct  \n",
       "25062  0.639491        0  \n",
       "28241  0.632091        0  \n",
       "9800   0.626192        0  \n",
       "13424  0.620598        0  \n",
       "2823   0.614451        0  \n",
       "17936  0.613466        0  \n",
       "14301  0.613301        0  \n",
       "3331   0.611929        0  \n",
       "3330   0.611929        0  \n",
       "13641  0.611702        0  \n",
       "15320  0.611249        0  \n",
       "8288   0.610032        0  \n",
       "3232   0.610006        0  \n",
       "566    0.609843        0  \n",
       "565    0.609843        0  \n",
       "25413  0.608871        0  \n",
       "26060  0.607676        0  \n",
       "26059  0.607676        0  \n",
       "5829   0.607619        0  \n",
       "5828   0.607619        0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Incorrect predictions with dis-similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using pipeline\n",
    "df_sub['score'] = test_preds_arr.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cases with duplicates scores\n",
    "df_sub['score'].count() - df_sub['score'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572927</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464975</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303382</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022798</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230748</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.130651</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.064500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.124117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.230264</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145350</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  score\n",
       "0  0.572927      2\n",
       "1  0.464975      2\n",
       "2  0.303382      2\n",
       "3  0.022798      2\n",
       "4  0.230748      2\n",
       "5  0.130651      2\n",
       "6  0.064500      2\n",
       "7  0.124117      2\n",
       "8  0.230264      2\n",
       "9  0.145350      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_score = df_sub['score'].value_counts().reset_index()[:10]\n",
    "same_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>95080362</td>\n",
       "      <td>\"\\n\\nPlease do not add nonsense to Wikipedia. ...</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>160935265</td>\n",
       "      <td>\"\\n\\nPlease do not add nonsense to Wikipedia. ...</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>275797183</td>\n",
       "      <td>Hi\\n\\nCould you please learn to interact like ...</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>275812977</td>\n",
       "      <td>Could you please learn to interact like a sent...</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>298854514</td>\n",
       "      <td>her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...</td>\n",
       "      <td>0.464975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>301925517</td>\n",
       "      <td>her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...</td>\n",
       "      <td>0.464975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>339478276</td>\n",
       "      <td>I'm gonna beat you to a bloody pulp then sho...</td>\n",
       "      <td>0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>339478966</td>\n",
       "      <td>I'm gonna beat you to a bloody pulp then shoo...</td>\n",
       "      <td>0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>345043812</td>\n",
       "      <td>JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A MO...</td>\n",
       "      <td>0.303382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>345043888</td>\n",
       "      <td>JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A M...</td>\n",
       "      <td>0.303382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>346641598</td>\n",
       "      <td>WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>346641762</td>\n",
       "      <td>WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>375083006</td>\n",
       "      <td>FC*K U\\n\\nWhy the fc*k should I get a warning ...</td>\n",
       "      <td>0.145350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>375157867</td>\n",
       "      <td>FC*K U\\n\\nWhy the fc*k should I get a warning ...</td>\n",
       "      <td>0.145350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>444588772</td>\n",
       "      <td>i will ki \\n\\nll you and wear your skin like ...</td>\n",
       "      <td>0.130651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>444589429</td>\n",
       "      <td>I will ki \\n\\nll you and wear your skin like ...</td>\n",
       "      <td>0.130651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>481969878</td>\n",
       "      <td>I will flay you alive, you fking stalker.  \\n...</td>\n",
       "      <td>0.230264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>481970432</td>\n",
       "      <td>I will flay you alive, you fking stalker. \\n\\...</td>\n",
       "      <td>0.230264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>501720037</td>\n",
       "      <td>Eat shit nigger \\n\\nI have infinite Ips I can...</td>\n",
       "      <td>0.572927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>501720124</td>\n",
       "      <td>Eat shit nigger \\n\\nI cant be blocked I have ...</td>\n",
       "      <td>0.572927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text     score\n",
       "1832    95080362  \"\\n\\nPlease do not add nonsense to Wikipedia. ...  0.022798\n",
       "2842   160935265  \"\\n\\nPlease do not add nonsense to Wikipedia. ...  0.022798\n",
       "4832   275797183  Hi\\n\\nCould you please learn to interact like ...  0.064500\n",
       "4833   275812977  Could you please learn to interact like a sent...  0.064500\n",
       "5140   298854514  her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...  0.464975\n",
       "5190   301925517  her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...  0.464975\n",
       "5752   339478276    I'm gonna beat you to a bloody pulp then sho...  0.230748\n",
       "5753   339478966   I'm gonna beat you to a bloody pulp then shoo...  0.230748\n",
       "5832   345043812  JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A MO...  0.303382\n",
       "5833   345043888   JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A M...  0.303382\n",
       "5852   346641598  WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...  0.124117\n",
       "5853   346641762  WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...  0.124117\n",
       "6193   375083006  FC*K U\\n\\nWhy the fc*k should I get a warning ...  0.145350\n",
       "6194   375157867  FC*K U\\n\\nWhy the fc*k should I get a warning ...  0.145350\n",
       "6947   444588772   i will ki \\n\\nll you and wear your skin like ...  0.130651\n",
       "6948   444589429   I will ki \\n\\nll you and wear your skin like ...  0.130651\n",
       "7312   481969878   I will flay you alive, you fking stalker.  \\n...  0.230264\n",
       "7313   481970432   I will flay you alive, you fking stalker. \\n\\...  0.230264\n",
       "7503   501720037   Eat shit nigger \\n\\nI have infinite Ips I can...  0.572927\n",
       "7504   501720124   Eat shit nigger \\n\\nI cant be blocked I have ...  0.572927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub['score'].isin(same_score['index'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>186197494</td>\n",
       "      <td>\"\\nFor copying and pasting of what I felt stro...</td>\n",
       "      <td>0.141626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>116257386</td>\n",
       "      <td>Dude!  \\nThat was an attempt at saying somethi...</td>\n",
       "      <td>0.160497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7070</th>\n",
       "      <td>457417171</td>\n",
       "      <td>You simply display your ignorance.  Fatuorum</td>\n",
       "      <td>0.274591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>242591983</td>\n",
       "      <td>\"\\n\\nSockpuppetry case\\n \\nYou have been accus...</td>\n",
       "      <td>0.012607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>70880071</td>\n",
       "      <td>Now let's see who's gonna start crying like a ...</td>\n",
       "      <td>0.279953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text     score\n",
       "3294   186197494  \"\\nFor copying and pasting of what I felt stro...  0.141626\n",
       "2167   116257386  Dude!  \\nThat was an attempt at saying somethi...  0.160497\n",
       "7070   457417171      You simply display your ignorance.  Fatuorum   0.274591\n",
       "4347   242591983  \"\\n\\nSockpuppetry case\\n \\nYou have been accus...  0.012607\n",
       "1370    70880071  Now let's see who's gonna start crying like a ...  0.279953"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission\n",
    "df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c9a7b156ba7ef82939fe85a8bbe37517a5f7091dd1ebbff4ae1cdb2f418c7af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('venv_py38': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
