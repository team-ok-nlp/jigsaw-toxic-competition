{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import clean, getData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(Dataset):\n",
    "    \"\"\"\n",
    "    get batch data and tokenizing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, is_eval=False) -> None:\n",
    "\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if is_eval:\n",
    "            self.df = clean(self.df)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # tokenize and to tensor\n",
    "        embedding = self.tokenizer.encode_plus(\n",
    "                                text=text,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\"\n",
    "                        )\n",
    "        \n",
    "        # print(text)\n",
    "        # print(embedding['input_ids'])\n",
    "        # print(embedding['token_type_ids'])\n",
    "        # print(embedding['attention_mask'])\n",
    "        return embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.df['comment'].values[idx]\n",
    "        labels = self.df['score'].values[idx]\n",
    "        # print(inputs)\n",
    "        # print(labels)\n",
    "\n",
    "        inputs = self.tokenize(inputs)\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self,model_name, num_class):\n",
    "\n",
    "        super(BertRegressor, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(768, num_class),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        outputs = self.bert(input_ids= input_id, attention_mask=mask)\n",
    "        outputs = self.regressor(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cofig\n",
    "CONFIG = dict(\n",
    "    seed = 12345,\n",
    "    pretrained_model = 'bert-base-uncased',\n",
    "    output_dir = '../models/bert_regression_test',\n",
    "    train_file = '4th/v0/train.csv',\n",
    "    dev_file = '4th/v0/dev.csv',\n",
    "    train_batch_size = 64,\n",
    "    dev_batch_size = 32,\n",
    "    lr = 5e-5,\n",
    "    epochs = 5,\n",
    "    num_class = 1,\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataloader, dev_dataloader, criterion, optimzer, scheduler, device):\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_loss_train = 0.0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                print(\"*********************\")\n",
    "                train_label = train_label.to(device)\n",
    "                print(\"*********************\")\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                print(\"*********************\")\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "                print(\"*********************\")\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                print(\n",
    "                    f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f}')\n",
    "\n",
    "\n",
    "            # validate using our dev set \n",
    "            total_loss_dev = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for dev_input, dev_label in dev_dataloader:\n",
    "                    dev_label = dev_label.to(device)\n",
    "                    input_id = dev_input['input_ids'].squeeze(1).to(device)\n",
    "                    mask = dev_input['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, dev_label)\n",
    "                    total_loss_dev += batch_loss.item()\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f} \\\n",
    "                | Val Loss: {total_loss_dev / len(dev_dataloader): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4th/v0/train.csv ...\n",
      "Read 4th/v0/dev.csv ...\n"
     ]
    }
   ],
   "source": [
    "# init bert pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['pretrained_model'])\n",
    "\n",
    "# load dataset\n",
    "# train dataset\n",
    "train_df = getData(data_path=CONFIG['train_file'])\n",
    "# data processing with tokenizing\n",
    "train_data = DataProcessor(train_df, tokenizer, is_eval=False)\n",
    "train_dataloader = DataLoader(train_data, batch_size=CONFIG['train_batch_size'], shuffle=True, num_workers=4)\n",
    "\n",
    "# dev dataset\n",
    "dev_df = getData(data_path=CONFIG['dev_file'])\n",
    "# data processing with tokenizing\n",
    "dev_data = DataProcessor(dev_df, tokenizer, is_eval=False)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=CONFIG['dev_batch_size'], shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertRegressor(CONFIG['pretrained_model'], CONFIG['num_class'])\n",
    "model.to(CONFIG['device'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['lr'],\n",
    "    eps=1e-8,\n",
    "    correct_bias=False) # AdamW to BERTAdam\n",
    "\n",
    "epochs = CONFIG['epochs']\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "num_warmup_steps = 10000\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=0, \n",
    "                num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27794 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "*********************\n",
      "*********************\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 23.70 GiB total capacity; 21.76 GiB already allocated; 319.00 MiB free; 21.86 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6af1b5e85e88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d8c99ffc8a36>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, train_dataloader, dev_dataloader, criterion, optimzer, scheduler, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*********************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cca20728d86c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 )\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     ):\n\u001b[0;32m--> 402\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 23.70 GiB total capacity; 21.76 GiB already allocated; 319.00 MiB free; 21.86 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "device = CONFIG['device']\n",
    "train(model, epochs, train_dataloader, dev_dataloader, criterion, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validataion and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       worker                                         less_toxic  \\\n",
      "10715     720   Good article \\nI just had an experience with ...   \n",
      "13722     524  DIFFERENT IP ADDRESS - 90.196.78.205   - SEE W...   \n",
      "1032      259   \\nListen now very clearfully to me you inuit1...   \n",
      "\n",
      "                                              more_toxic  \n",
      "10715  Some idiot named Dick Clark keeps on writing f...  \n",
      "13722  Devil Incarnate==\\nMany people have speculated...  \n",
      "1032                    Go to hell you ignorant asshole!  \n",
      "      comment_id                                               text\n",
      "4197   231716785  I agree with the Colonel and think he is a gen...\n",
      "6658   421658106   I have nothing but contempt for such people, ...\n",
      "5264   306545794      GFY loser. Budi takut bermain bola..... Ha ha\n"
     ]
    }
   ],
   "source": [
    "# Validation data \n",
    "df_val = getData(data_path=\"4th/validation_data.csv\")\n",
    "# Test data\n",
    "df_sub = getData(data_path=\"4th/comments_to_score.csv\")\n",
    "print(df_val.sample(3))\n",
    "print(df_sub.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  import sys\n",
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n",
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n",
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/donghoon/.pyenv/versions/3.7.7/envs/venv_py37/lib/python3.7/site-packages/ipykernel_launcher.py:27: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "df_val = clean(df_val, 'less_toxic')\n",
    "df_val = clean(df_val, 'more_toxic')\n",
    "df_sub = clean(df_sub, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation\n",
    "- final validation and submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['p1'] = p1\n",
    "df_val['p2'] = p2\n",
    "df_val['diff'] = np.abs(p2 - p1)\n",
    "df_val['correct'] = (p1 < p2).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>correct</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24702</th>\n",
       "      <td>204</td>\n",
       "      <td>This page is so conventional. It's sources are...</td>\n",
       "      <td>\"\\n\\nA more important point about Vuia is that...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.126276</td>\n",
       "      <td>0.126274</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>605</td>\n",
       "      <td>Well... D I cant say I will join in on the I w...</td>\n",
       "      <td>Sexism \\n\\nI've tried my hardest to explain t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040171</td>\n",
       "      <td>0.040115</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9703</th>\n",
       "      <td>515</td>\n",
       "      <td>Well... D I cant say I will join in on the I w...</td>\n",
       "      <td>Sexism \\n\\nI've tried my hardest to explain t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040171</td>\n",
       "      <td>0.040115</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>220</td>\n",
       "      <td>\"\\nI would like to agree with you, and maybe I...</td>\n",
       "      <td>THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150288</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>387</td>\n",
       "      <td>\"\\nI would like to agree with you, and maybe I...</td>\n",
       "      <td>THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150288</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4570</th>\n",
       "      <td>441</td>\n",
       "      <td>\"\\nI would like to agree with you, and maybe I...</td>\n",
       "      <td>THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150288</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26873</th>\n",
       "      <td>451</td>\n",
       "      <td>JLaTondre is a gay boy who likes to suck dick...</td>\n",
       "      <td>Thank You your the Wellcome\\nARABIC ASSHOLR GO...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363390</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>216</td>\n",
       "      <td>\"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...</td>\n",
       "      <td>Hey RGT: you're not just simply disagreeing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129691</td>\n",
       "      <td>0.129503</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>168</td>\n",
       "      <td>\"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...</td>\n",
       "      <td>Hey RGT: you're not just simply disagreeing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129691</td>\n",
       "      <td>0.129503</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>180</td>\n",
       "      <td>\"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...</td>\n",
       "      <td>Hey RGT: you're not just simply disagreeing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129691</td>\n",
       "      <td>0.129503</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>254</td>\n",
       "      <td>hiding behind these bitches wikipedians who ke...</td>\n",
       "      <td>You are an arrogant little so-and-so - aren't ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297834</td>\n",
       "      <td>0.297586</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>256</td>\n",
       "      <td>September 17th vandalized \\n\\nThis date which...</td>\n",
       "      <td>\"\\n\\nSorry, being a fan of wrestling I would a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>0.013654</td>\n",
       "      <td>0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20229</th>\n",
       "      <td>209</td>\n",
       "      <td>Absolutely hysterical.  Go get consensus on th...</td>\n",
       "      <td>Beckenham Library user blocked on public comp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071487</td>\n",
       "      <td>0.071217</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>302</td>\n",
       "      <td>\"\\n\\nRebuild of Evangelion revert warring\\n\\nT...</td>\n",
       "      <td>\"\\n\\n A barnstar for you! \\n\\n  The Original B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072787</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>300</td>\n",
       "      <td>\"\\n\\nRebuild of Evangelion revert warring\\n\\nT...</td>\n",
       "      <td>\"\\n\\n A barnstar for you! \\n\\n  The Original B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072787</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11725</th>\n",
       "      <td>572</td>\n",
       "      <td>User:Adam1213/warn&gt;\\n\\n Vandalism warnings</td>\n",
       "      <td>A Moldavian\\nI am a Moldavian, and I can expla...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>-0.009871</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11724</th>\n",
       "      <td>732</td>\n",
       "      <td>User:Adam1213/warn&gt;\\n\\n Vandalism warnings</td>\n",
       "      <td>A Moldavian\\nI am a Moldavian, and I can expla...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>-0.009871</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10382</th>\n",
       "      <td>489</td>\n",
       "      <td>This user has a history of sock puppetry and l...</td>\n",
       "      <td>\"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10381</th>\n",
       "      <td>171</td>\n",
       "      <td>This user has a history of sock puppetry and l...</td>\n",
       "      <td>\"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10380</th>\n",
       "      <td>22</td>\n",
       "      <td>This user has a history of sock puppetry and l...</td>\n",
       "      <td>\"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "24702     204  This page is so conventional. It's sources are...   \n",
       "9704      605  Well... D I cant say I will join in on the I w...   \n",
       "9703      515  Well... D I cant say I will join in on the I w...   \n",
       "4571      220  \"\\nI would like to agree with you, and maybe I...   \n",
       "4569      387  \"\\nI would like to agree with you, and maybe I...   \n",
       "4570      441  \"\\nI would like to agree with you, and maybe I...   \n",
       "26873     451   JLaTondre is a gay boy who likes to suck dick...   \n",
       "1934      216  \"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...   \n",
       "1935      168  \"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...   \n",
       "1936      180  \"\\n\\nMorrison's \"\"Black President\"\" Phrase Was...   \n",
       "27670     254  hiding behind these bitches wikipedians who ke...   \n",
       "9640      256   September 17th vandalized \\n\\nThis date which...   \n",
       "20229     209  Absolutely hysterical.  Go get consensus on th...   \n",
       "195       302  \"\\n\\nRebuild of Evangelion revert warring\\n\\nT...   \n",
       "196       300  \"\\n\\nRebuild of Evangelion revert warring\\n\\nT...   \n",
       "11725     572        User:Adam1213/warn>\\n\\n Vandalism warnings    \n",
       "11724     732        User:Adam1213/warn>\\n\\n Vandalism warnings    \n",
       "10382     489  This user has a history of sock puppetry and l...   \n",
       "10381     171  This user has a history of sock puppetry and l...   \n",
       "10380      22  This user has a history of sock puppetry and l...   \n",
       "\n",
       "                                              more_toxic  correct        p1  \\\n",
       "24702  \"\\n\\nA more important point about Vuia is that...        0  0.126276   \n",
       "9704    Sexism \\n\\nI've tried my hardest to explain t...        0  0.040171   \n",
       "9703    Sexism \\n\\nI've tried my hardest to explain t...        0  0.040171   \n",
       "4571    THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...        0  0.150288   \n",
       "4569    THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...        0  0.150288   \n",
       "4570    THEIR GOING TO BATH \\n\\nBUT THEY'RE A MUSLIM ...        0  0.150288   \n",
       "26873  Thank You your the Wellcome\\nARABIC ASSHOLR GO...        0  0.363390   \n",
       "1934    Hey RGT: you're not just simply disagreeing. ...        0  0.129691   \n",
       "1935    Hey RGT: you're not just simply disagreeing. ...        0  0.129691   \n",
       "1936    Hey RGT: you're not just simply disagreeing. ...        0  0.129691   \n",
       "27670  You are an arrogant little so-and-so - aren't ...        0  0.297834   \n",
       "9640   \"\\n\\nSorry, being a fan of wrestling I would a...        0  0.013912   \n",
       "20229   Beckenham Library user blocked on public comp...        0  0.071487   \n",
       "195    \"\\n\\n A barnstar for you! \\n\\n  The Original B...        0  0.072787   \n",
       "196    \"\\n\\n A barnstar for you! \\n\\n  The Original B...        0  0.072787   \n",
       "11725  A Moldavian\\nI am a Moldavian, and I can expla...        0 -0.009541   \n",
       "11724  A Moldavian\\nI am a Moldavian, and I can expla...        0 -0.009541   \n",
       "10382  \"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...        0  0.062868   \n",
       "10381  \"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...        0  0.062868   \n",
       "10380  \"\\n\\nBarnstar\\n\\nSgrayban, for your hard work ...        0  0.062868   \n",
       "\n",
       "             p2      diff  \n",
       "24702  0.126274  0.000002  \n",
       "9704   0.040115  0.000056  \n",
       "9703   0.040115  0.000056  \n",
       "4571   0.150198  0.000090  \n",
       "4569   0.150198  0.000090  \n",
       "4570   0.150198  0.000090  \n",
       "26873  0.363255  0.000136  \n",
       "1934   0.129503  0.000188  \n",
       "1935   0.129503  0.000188  \n",
       "1936   0.129503  0.000188  \n",
       "27670  0.297586  0.000248  \n",
       "9640   0.013654  0.000259  \n",
       "20229  0.071217  0.000269  \n",
       "195    0.072501  0.000286  \n",
       "196    0.072501  0.000286  \n",
       "11725 -0.009871  0.000330  \n",
       "11724 -0.009871  0.000330  \n",
       "10382  0.062414  0.000453  \n",
       "10381  0.062414  0.000453  \n",
       "10380  0.062414  0.000453  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Incorrect predictions with similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>correct</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>246</td>\n",
       "      <td>CaliforniaAliBaba is a Bitch\\nGo fuck yourself...</td>\n",
       "      <td>I've heard a snippet on their myspace page but...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.275426</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>1.277321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28241</th>\n",
       "      <td>118</td>\n",
       "      <td>you are a stupid fuck \\n\\nand your mother's c...</td>\n",
       "      <td>\"\\n\\n  :::   According to Stratfordian authori...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.184728</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>1.134825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>431</td>\n",
       "      <td>ASSHOLES! COME AND TRY ME YOU STUPID FUCK SON ...</td>\n",
       "      <td>\"\\n\\nPlease, Cheesebot is not nonsense. It is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.215474</td>\n",
       "      <td>0.112650</td>\n",
       "      <td>1.102823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19411</th>\n",
       "      <td>46</td>\n",
       "      <td>Ok, what is it \\n\\nWhy are you such a queer.....</td>\n",
       "      <td>wow the guy who deleated the stuff on the Habb...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.161545</td>\n",
       "      <td>0.069165</td>\n",
       "      <td>1.092379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21059</th>\n",
       "      <td>333</td>\n",
       "      <td>fuck you \\n\\nyou're an idiot</td>\n",
       "      <td>What makes you an editor? Why do you use the n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.125096</td>\n",
       "      <td>0.070252</td>\n",
       "      <td>1.054844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18002</th>\n",
       "      <td>634</td>\n",
       "      <td>Are you lost, stupid fucking idiots</td>\n",
       "      <td>Appeal \\n\\n{{unblock|G'day everyone. First, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.124004</td>\n",
       "      <td>0.075780</td>\n",
       "      <td>1.048224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7788</th>\n",
       "      <td>580</td>\n",
       "      <td>suck my dick \\n\\nfuck you dirty whore bitch</td>\n",
       "      <td>Stop being such a sissy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.362443</td>\n",
       "      <td>0.319210</td>\n",
       "      <td>1.043232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11226</th>\n",
       "      <td>320</td>\n",
       "      <td>I fucked your dad last night you fucking whor...</td>\n",
       "      <td>Please don't try and mark a page for deletion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.071216</td>\n",
       "      <td>0.067594</td>\n",
       "      <td>1.003623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191</th>\n",
       "      <td>504</td>\n",
       "      <td>cunt. I hate your face. You are a fucking shit...</td>\n",
       "      <td>\"\\n\\nAll my uploaded pdf-documents have been u...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.152207</td>\n",
       "      <td>0.158813</td>\n",
       "      <td>0.993394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25754</th>\n",
       "      <td>190</td>\n",
       "      <td>what the hell man? why did you delete my page ...</td>\n",
       "      <td>\" December 2009 (UTC)\\n\\n Now there's typical ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.186038</td>\n",
       "      <td>0.199026</td>\n",
       "      <td>0.987012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20435</th>\n",
       "      <td>504</td>\n",
       "      <td>Fuck you niggerkite. y you deleting so much sh...</td>\n",
       "      <td>Bush is the man. \\n\\nHe had the balls to disp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.117708</td>\n",
       "      <td>0.131863</td>\n",
       "      <td>0.985845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>733</td>\n",
       "      <td>Steve Little \\n\\nSteve Little's character in ...</td>\n",
       "      <td>The only thing I can considor I an attack was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985752</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.983224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9175</th>\n",
       "      <td>439</td>\n",
       "      <td>\\nPENIS COCKWAD NIGGA NIGGA NYUGGGGAAA!!! NIG...</td>\n",
       "      <td>\"\\n\\nConstance Cumbey Update on March 5, 2005 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.015603</td>\n",
       "      <td>0.039863</td>\n",
       "      <td>0.975740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17193</th>\n",
       "      <td>238</td>\n",
       "      <td>fuck this this is bull shit and it does not ev...</td>\n",
       "      <td>Just To Let You Know. You have no life.\\nI hat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>0.213855</td>\n",
       "      <td>0.972487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>347</td>\n",
       "      <td>fuck this this is bull shit and it does not ev...</td>\n",
       "      <td>Just To Let You Know. You have no life.\\nI hat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>0.213855</td>\n",
       "      <td>0.972487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9227</th>\n",
       "      <td>78</td>\n",
       "      <td>Yea well kiss my fucker shit nuts!</td>\n",
       "      <td>Azari or Azerbaijani? \\n\\nAzari-iranian,azerb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.939252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29057</th>\n",
       "      <td>716</td>\n",
       "      <td>Fuck You\\n\\nGo suck on a fat dick faggot mothe...</td>\n",
       "      <td>\"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.270009</td>\n",
       "      <td>0.338313</td>\n",
       "      <td>0.931695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29056</th>\n",
       "      <td>204</td>\n",
       "      <td>Fuck You\\n\\nGo suck on a fat dick faggot mothe...</td>\n",
       "      <td>\"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.270009</td>\n",
       "      <td>0.338313</td>\n",
       "      <td>0.931695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22221</th>\n",
       "      <td>441</td>\n",
       "      <td>Green Tea and CigarettesWhy you're a worthles...</td>\n",
       "      <td>It doesn't offend me, but that's the thing abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.178067</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>0.931257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22222</th>\n",
       "      <td>254</td>\n",
       "      <td>Green Tea and CigarettesWhy you're a worthles...</td>\n",
       "      <td>It doesn't offend me, but that's the thing abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.178067</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>0.931257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "25062     246  CaliforniaAliBaba is a Bitch\\nGo fuck yourself...   \n",
       "28241     118   you are a stupid fuck \\n\\nand your mother's c...   \n",
       "1952      431  ASSHOLES! COME AND TRY ME YOU STUPID FUCK SON ...   \n",
       "19411      46   Ok, what is it \\n\\nWhy are you such a queer.....   \n",
       "21059     333                       fuck you \\n\\nyou're an idiot   \n",
       "18002     634                Are you lost, stupid fucking idiots   \n",
       "7788      580        suck my dick \\n\\nfuck you dirty whore bitch   \n",
       "11226     320   I fucked your dad last night you fucking whor...   \n",
       "20191     504  cunt. I hate your face. You are a fucking shit...   \n",
       "25754     190  what the hell man? why did you delete my page ...   \n",
       "20435     504  Fuck you niggerkite. y you deleting so much sh...   \n",
       "12482     733   Steve Little \\n\\nSteve Little's character in ...   \n",
       "9175      439   \\nPENIS COCKWAD NIGGA NIGGA NYUGGGGAAA!!! NIG...   \n",
       "17193     238  fuck this this is bull shit and it does not ev...   \n",
       "17192     347  fuck this this is bull shit and it does not ev...   \n",
       "9227       78                 Yea well kiss my fucker shit nuts!   \n",
       "29057     716  Fuck You\\n\\nGo suck on a fat dick faggot mothe...   \n",
       "29056     204  Fuck You\\n\\nGo suck on a fat dick faggot mothe...   \n",
       "22221     441   Green Tea and CigarettesWhy you're a worthles...   \n",
       "22222     254   Green Tea and CigarettesWhy you're a worthles...   \n",
       "\n",
       "                                              more_toxic  correct        p1  \\\n",
       "25062  I've heard a snippet on their myspace page but...        0  1.275426   \n",
       "28241  \"\\n\\n  :::   According to Stratfordian authori...        0  1.184728   \n",
       "1952   \"\\n\\nPlease, Cheesebot is not nonsense. It is ...        0  1.215474   \n",
       "19411  wow the guy who deleated the stuff on the Habb...        0  1.161545   \n",
       "21059  What makes you an editor? Why do you use the n...        0  1.125096   \n",
       "18002   Appeal \\n\\n{{unblock|G'day everyone. First, I...        0  1.124004   \n",
       "7788                           Stop being such a sissy.         0  1.362443   \n",
       "11226  Please don't try and mark a page for deletion ...        0  1.071216   \n",
       "20191  \"\\n\\nAll my uploaded pdf-documents have been u...        0  1.152207   \n",
       "25754  \" December 2009 (UTC)\\n\\n Now there's typical ...        0  1.186038   \n",
       "20435   Bush is the man. \\n\\nHe had the balls to disp...        0  1.117708   \n",
       "12482   The only thing I can considor I an attack was...        0  0.985752   \n",
       "9175   \"\\n\\nConstance Cumbey Update on March 5, 2005 ...        0  1.015603   \n",
       "17193  Just To Let You Know. You have no life.\\nI hat...        0  1.186342   \n",
       "17192  Just To Let You Know. You have no life.\\nI hat...        0  1.186342   \n",
       "9227    Azari or Azerbaijani? \\n\\nAzari-iranian,azerb...        0  0.943498   \n",
       "29057  \"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...        0  1.270009   \n",
       "29056  \"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK H...        0  1.270009   \n",
       "22221  It doesn't offend me, but that's the thing abo...        0  1.178067   \n",
       "22222  It doesn't offend me, but that's the thing abo...        0  1.178067   \n",
       "\n",
       "             p2      diff  \n",
       "25062 -0.001895  1.277321  \n",
       "28241  0.049904  1.134825  \n",
       "1952   0.112650  1.102823  \n",
       "19411  0.069165  1.092379  \n",
       "21059  0.070252  1.054844  \n",
       "18002  0.075780  1.048224  \n",
       "7788   0.319210  1.043232  \n",
       "11226  0.067594  1.003623  \n",
       "20191  0.158813  0.993394  \n",
       "25754  0.199026  0.987012  \n",
       "20435  0.131863  0.985845  \n",
       "12482  0.002528  0.983224  \n",
       "9175   0.039863  0.975740  \n",
       "17193  0.213855  0.972487  \n",
       "17192  0.213855  0.972487  \n",
       "9227   0.004246  0.939252  \n",
       "29057  0.338313  0.931695  \n",
       "29056  0.338313  0.931695  \n",
       "22221  0.246810  0.931257  \n",
       "22222  0.246810  0.931257  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Incorrect predictions with dis-similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using pipeline\n",
    "df_sub['score'] = test_preds_arr.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cases with duplicates scores\n",
    "df_sub['score'].count() - df_sub['score'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572927</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464975</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303382</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022798</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230748</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.130651</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.064500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.124117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.230264</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145350</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  score\n",
       "0  0.572927      2\n",
       "1  0.464975      2\n",
       "2  0.303382      2\n",
       "3  0.022798      2\n",
       "4  0.230748      2\n",
       "5  0.130651      2\n",
       "6  0.064500      2\n",
       "7  0.124117      2\n",
       "8  0.230264      2\n",
       "9  0.145350      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_score = df_sub['score'].value_counts().reset_index()[:10]\n",
    "same_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>95080362</td>\n",
       "      <td>\"\\n\\nPlease do not add nonsense to Wikipedia. ...</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>160935265</td>\n",
       "      <td>\"\\n\\nPlease do not add nonsense to Wikipedia. ...</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>275797183</td>\n",
       "      <td>Hi\\n\\nCould you please learn to interact like ...</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>275812977</td>\n",
       "      <td>Could you please learn to interact like a sent...</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>298854514</td>\n",
       "      <td>her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...</td>\n",
       "      <td>0.464975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>301925517</td>\n",
       "      <td>her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...</td>\n",
       "      <td>0.464975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>339478276</td>\n",
       "      <td>I'm gonna beat you to a bloody pulp then sho...</td>\n",
       "      <td>0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>339478966</td>\n",
       "      <td>I'm gonna beat you to a bloody pulp then shoo...</td>\n",
       "      <td>0.230748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>345043812</td>\n",
       "      <td>JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A MO...</td>\n",
       "      <td>0.303382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>345043888</td>\n",
       "      <td>JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A M...</td>\n",
       "      <td>0.303382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>346641598</td>\n",
       "      <td>WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>346641762</td>\n",
       "      <td>WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>375083006</td>\n",
       "      <td>FC*K U\\n\\nWhy the fc*k should I get a warning ...</td>\n",
       "      <td>0.145350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>375157867</td>\n",
       "      <td>FC*K U\\n\\nWhy the fc*k should I get a warning ...</td>\n",
       "      <td>0.145350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>444588772</td>\n",
       "      <td>i will ki \\n\\nll you and wear your skin like ...</td>\n",
       "      <td>0.130651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>444589429</td>\n",
       "      <td>I will ki \\n\\nll you and wear your skin like ...</td>\n",
       "      <td>0.130651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>481969878</td>\n",
       "      <td>I will flay you alive, you fking stalker.  \\n...</td>\n",
       "      <td>0.230264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>481970432</td>\n",
       "      <td>I will flay you alive, you fking stalker. \\n\\...</td>\n",
       "      <td>0.230264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>501720037</td>\n",
       "      <td>Eat shit nigger \\n\\nI have infinite Ips I can...</td>\n",
       "      <td>0.572927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>501720124</td>\n",
       "      <td>Eat shit nigger \\n\\nI cant be blocked I have ...</td>\n",
       "      <td>0.572927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text     score\n",
       "1832    95080362  \"\\n\\nPlease do not add nonsense to Wikipedia. ...  0.022798\n",
       "2842   160935265  \"\\n\\nPlease do not add nonsense to Wikipedia. ...  0.022798\n",
       "4832   275797183  Hi\\n\\nCould you please learn to interact like ...  0.064500\n",
       "4833   275812977  Could you please learn to interact like a sent...  0.064500\n",
       "5140   298854514  her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...  0.464975\n",
       "5190   301925517  her!\\n\\nPoop, pee, toot, fart, gas, diareah!\\n...  0.464975\n",
       "5752   339478276    I'm gonna beat you to a bloody pulp then sho...  0.230748\n",
       "5753   339478966   I'm gonna beat you to a bloody pulp then shoo...  0.230748\n",
       "5832   345043812  JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A MO...  0.303382\n",
       "5833   345043888   JIMBO SAID I COULD EDIT HIS PAGE. YOU ARE A M...  0.303382\n",
       "5852   346641598  WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...  0.124117\n",
       "5853   346641762  WE ARE GOING TO MAKE SURE YOU ARE EXPOSED!! DR...  0.124117\n",
       "6193   375083006  FC*K U\\n\\nWhy the fc*k should I get a warning ...  0.145350\n",
       "6194   375157867  FC*K U\\n\\nWhy the fc*k should I get a warning ...  0.145350\n",
       "6947   444588772   i will ki \\n\\nll you and wear your skin like ...  0.130651\n",
       "6948   444589429   I will ki \\n\\nll you and wear your skin like ...  0.130651\n",
       "7312   481969878   I will flay you alive, you fking stalker.  \\n...  0.230264\n",
       "7313   481970432   I will flay you alive, you fking stalker. \\n\\...  0.230264\n",
       "7503   501720037   Eat shit nigger \\n\\nI have infinite Ips I can...  0.572927\n",
       "7504   501720124   Eat shit nigger \\n\\nI cant be blocked I have ...  0.572927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub[df_sub['score'].isin(same_score['index'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>186197494</td>\n",
       "      <td>\"\\nFor copying and pasting of what I felt stro...</td>\n",
       "      <td>0.141626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>116257386</td>\n",
       "      <td>Dude!  \\nThat was an attempt at saying somethi...</td>\n",
       "      <td>0.160497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7070</th>\n",
       "      <td>457417171</td>\n",
       "      <td>You simply display your ignorance.  Fatuorum</td>\n",
       "      <td>0.274591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>242591983</td>\n",
       "      <td>\"\\n\\nSockpuppetry case\\n \\nYou have been accus...</td>\n",
       "      <td>0.012607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>70880071</td>\n",
       "      <td>Now let's see who's gonna start crying like a ...</td>\n",
       "      <td>0.279953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text     score\n",
       "3294   186197494  \"\\nFor copying and pasting of what I felt stro...  0.141626\n",
       "2167   116257386  Dude!  \\nThat was an attempt at saying somethi...  0.160497\n",
       "7070   457417171      You simply display your ignorance.  Fatuorum   0.274591\n",
       "4347   242591983  \"\\n\\nSockpuppetry case\\n \\nYou have been accus...  0.012607\n",
       "1370    70880071  Now let's see who's gonna start crying like a ...  0.279953"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission\n",
    "df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c9a7b156ba7ef82939fe85a8bbe37517a5f7091dd1ebbff4ae1cdb2f418c7af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('venv_py38': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
